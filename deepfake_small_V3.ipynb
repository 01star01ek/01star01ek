{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "An3Kh3m69Jqo"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/01star01ek/01star01ek/blob/main/deepfake_small_V3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# install dependency"
      ],
      "metadata": {
        "id": "An3Kh3m69Jqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe==0.10.11\n",
        "!pip install opencv-contrib-python flatbuffers==23.5.26 sounddevice==0.4.6 attrs==23.1.0\n",
        "!pip install torch==2.1.0 torchvision==0.16.0\n",
        "!pip install dlib opencv-python scikit-image pillow matplotlib imageio gdown tqdm\n",
        "!pip install ninja tensorboard tensorboardX pyaml pyrallis ftfy\n",
        "!pip install face-alignment==1.3.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ah4vUmMoLXIo",
        "outputId": "9880f383-a7ee-47f3-d557-9de4ffbf857c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe==0.10.11\n",
            "  Downloading mediapipe-0.10.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.11) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.11) (25.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.11) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.11) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.11) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.11) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.11) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.11) (2.5.1+cu124)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.11) (4.11.0.86)\n",
            "Collecting protobuf<4,>=3.11 (from mediapipe==0.10.11)\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe==0.10.11)\n",
            "  Downloading sounddevice-0.5.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe==0.10.11) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe==0.10.11) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe==0.10.11) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe==0.10.11) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.11) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.11) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.11) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.11) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.11) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.11) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.11) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.11) (2.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe==0.10.11) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe==0.10.11) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe==0.10.11) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe==0.10.11) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe==0.10.11) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->mediapipe==0.10.11)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->mediapipe==0.10.11)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->mediapipe==0.10.11)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->mediapipe==0.10.11)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->mediapipe==0.10.11)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->mediapipe==0.10.11)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->mediapipe==0.10.11)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->mediapipe==0.10.11)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->mediapipe==0.10.11)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe==0.10.11) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe==0.10.11) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->mediapipe==0.10.11)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe==0.10.11) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe==0.10.11) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->mediapipe==0.10.11) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe==0.10.11) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe==0.10.11) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->mediapipe==0.10.11) (3.0.2)\n",
            "Downloading mediapipe-0.10.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.2-py3-none-any.whl (32 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m155.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.4 --force-reinstall"
      ],
      "metadata": {
        "id": "j6kl565aMbdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/extract_frames\n",
        "!mkdir -p /content/input_videos\n",
        "!mkdir -p /content/alignmented_frame\n",
        "!mkdir -p /content/alignmented_frame_aligned\n",
        "!mkdir -p /content/alignmented_frame_croped\n",
        "!mkdir -p /content/alignmented_frame_transforms"
      ],
      "metadata": {
        "id": "UlYQQ1my-rL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#git hub & model install"
      ],
      "metadata": {
        "id": "Io1UJS2j-HYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/yuval-alaluf/stylegan3-editing.git"
      ],
      "metadata": {
        "id": "Ae5jdZJn-UUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## models"
      ],
      "metadata": {
        "id": "8gQABgnW-aOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 -P /content/pretrained_models/\n",
        "!bzip2 -d /content/pretrained_models/shape_predictor_68_face_landmarks.dat.bz2"
      ],
      "metadata": {
        "id": "LBZNpi4s-HAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1z_cB187QOc6aqVBdLvYvBjoc93-_EuRm -O /content/pretrained_models/restyle_e4e_sg3.pt\n",
        "!gdown --id 13q6m-bpe3Ws9en9y45JEx2PHQirStt8N -O /content/pretrained_models/stylegan3-ffhq-1024x1024.pt\n",
        "!gdown --id 1KW7bjndL3QG3sxBbZxreGHigcCCpsDgn -O /content/pretrained_models/model_ir_se50.pth"
      ],
      "metadata": {
        "id": "ntmFmSfd-ZiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/stylegan3-editing/pretrained_models\n",
        "!cp /content/pretrained_models/shape_predictor_68_face_landmarks.dat /content/stylegan3-editing/pretrained_models/"
      ],
      "metadata": {
        "id": "aCa0uxct-mxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# preprocess frames"
      ],
      "metadata": {
        "id": "PBElw7K4_zJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ í”„ë ˆì„ ì¶”ì¶œ ì½”ë“œ\n",
        "import cv2\n",
        "import os\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "video_dir = \"/content/input_videos\"  #@param {type:\"string\"}\n",
        "output_base = \"/content/extract_frames\"  #@param {type:\"string\"}\n",
        "extract_per_sec = 7  #@param {type:\"integer\"}\n",
        "\n",
        "os.makedirs(output_base, exist_ok=True)\n",
        "\n",
        "def extract_video_frames(video_path):\n",
        "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "    output_dir = os.path.join(output_base, video_name)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    interval = max(1, int(fps // extract_per_sec))\n",
        "\n",
        "    frame_idx = 0\n",
        "    frame_list = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if frame_idx % interval == 0:\n",
        "            frame_list.append(frame)\n",
        "        frame_idx += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # ì—¬ê¸°ì„œ í•œ ë²ˆì— ì €ì¥!\n",
        "    for saved_idx, frame in enumerate(frame_list):\n",
        "        frame_path = os.path.join(output_dir, f\"key_{saved_idx:04d}.jpg\")\n",
        "        cv2.imwrite(frame_path, frame)\n",
        "\n",
        "    return f\"{video_name}: {len(frame_list)} frames saved\"\n",
        "\n",
        "# ì‹¤í–‰\n",
        "video_paths = glob(os.path.join(video_dir, \"*.mp4\"))\n",
        "print(f\"ğŸ¬ ì´ {len(video_paths)}ê°œì˜ ì˜ìƒ ì²˜ë¦¬ ì‹œì‘\")\n",
        "\n",
        "for video_path in tqdm(video_paths, desc=\"ğŸ“¦ Processing videos\"):\n",
        "    msg = extract_video_frames(video_path)"
      ],
      "metadata": {
        "id": "7U3XEaKpDfN2",
        "cellView": "code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title í”¼ì¹˜ ì œí•œ ê°•í™” + ëˆˆëœ¸ 50% ê¸°ì¤€ + KeyError í•´ê²° ì½”ë“œ\n",
        "import cv2, os, math, numpy as np, pandas as pd\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import mediapipe as mp\n",
        "\n",
        "# MediaPipe ì´ˆê¸°í™”\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "FACE_MESH = mp_face_mesh.FaceMesh(\n",
        "    static_image_mode=True,\n",
        "    max_num_faces=1,\n",
        "    refine_landmarks=True,\n",
        "    min_detection_confidence=0.7,\n",
        "    min_tracking_confidence=0.5\n",
        ")\n",
        "\n",
        "# ëˆˆ ì¢Œí‘œ ì¸ë±ìŠ¤\n",
        "LEFT_EYE_IDX = [\n",
        "    33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161, 246,\n",
        "    33, 173, 157, 158, 159, 160, 161, 246, 33\n",
        "]\n",
        "RIGHT_EYE_IDX = [\n",
        "    362, 398, 384, 385, 386, 387, 388, 466, 263, 249, 390, 373, 374, 380, 381, 382,\n",
        "    362, 398, 384, 385, 386, 387, 388, 466, 362\n",
        "]\n",
        "\n",
        "CORE_LEFT_EYE = [33, 160, 158, 133, 153, 144]\n",
        "CORE_RIGHT_EYE = [362, 385, 387, 263, 373, 380]\n",
        "LEFT_EYE_VERTICAL = [159, 145]\n",
        "RIGHT_EYE_VERTICAL = [386, 374]\n",
        "POSE_IDX = [1, 152, 33, 263, 61, 291]\n",
        "\n",
        "# íŒŒë¼ë¯¸í„° ì„¤ì • - í”¼ì¹˜ ì œí•œ ê°•í™”, ëˆˆëœ¸ ê¸°ì¤€ ì™„í™”\n",
        "YAW_T = 25\n",
        "PITCH_T = 25  # 35ì—ì„œ 25ë¡œ ê°•í™” (í”¼ì¹˜ ê´€ëŒ€í•˜ê²Œ í•˜ì§€ ì•ŠìŒ)\n",
        "HEAD_DOWN_BONUS = 1  # ë³´ë„ˆìŠ¤ ìµœì†Œí™”\n",
        "ENABLE_ADAPTIVE_EAR = True\n",
        "EAR_PERCENTILE_HIGH = 37 # ìƒìœ„ 50%ë¥¼ ëˆˆëœ¬ ìƒíƒœë¡œ íŒì • (ê¸°ì¡´ 40%ì—ì„œ ì™„í™”)\n",
        "EAR_PERCENTILE_LOW = 10\n",
        "\n",
        "model_points = np.array([\n",
        "    (0.0, 0.0, 0.0),\n",
        "    (0.0, -330.0, -65.0),\n",
        "    (-225.0, 170.0, -135.0),\n",
        "    (225.0, 170.0, -135.0),\n",
        "    (-150.0, -150.0, -125.0),\n",
        "    (150.0, -150.0, -125.0)\n",
        "], dtype=\"double\")\n",
        "\n",
        "def get_mediapipe_landmarks(img):\n",
        "    h, w = img.shape[:2]\n",
        "    rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    res = FACE_MESH.process(rgb)\n",
        "    if not res.multi_face_landmarks:\n",
        "        return None\n",
        "    lm = res.multi_face_landmarks[0]\n",
        "    coords = np.array([[p.x * w, p.y * h] for p in lm.landmark])\n",
        "    return coords\n",
        "\n",
        "def estimate_pose_mediapipe(landmarks, img_shape):\n",
        "    image_points = landmarks[POSE_IDX]\n",
        "    focal = img_shape[1]\n",
        "    center = (img_shape[1]/2, img_shape[0]/2)\n",
        "    cam = np.array([[focal, 0, center[0]], [0, focal, center[1]], [0, 0, 1]], dtype=\"double\")\n",
        "    dist = np.zeros((4,1))\n",
        "    success, rv, _ = cv2.solvePnP(model_points, image_points, cam, dist)\n",
        "    return rv if success else None\n",
        "\n",
        "def rotation_vector_to_euler(rv):\n",
        "    rmat, _ = cv2.Rodrigues(rv)\n",
        "    proj = np.hstack((rmat, np.zeros((3,1))))\n",
        "    angles = cv2.decomposeProjectionMatrix(proj)[6]\n",
        "    pitch = math.degrees(math.asin(math.sin(math.radians(angles[1][0]))))\n",
        "    yaw   = math.degrees(math.asin(math.sin(math.radians(angles[2][0]))))\n",
        "    roll  = -math.degrees(math.asin(math.sin(math.radians(angles[0][0]))))\n",
        "    return pitch, yaw, roll\n",
        "\n",
        "def eye_aspect_ratio(eye):\n",
        "    A = np.linalg.norm(eye[1] - eye[5])\n",
        "    B = np.linalg.norm(eye[2] - eye[4])\n",
        "    C = np.linalg.norm(eye[0] - eye[3])\n",
        "    return (A + B) / (2.0 * C)\n",
        "\n",
        "def enhanced_eye_aspect_ratio(landmarks, is_left=True):\n",
        "    if is_left:\n",
        "        outer = landmarks[33]\n",
        "        inner = landmarks[133]\n",
        "        v1 = np.linalg.norm(landmarks[159] - landmarks[145])\n",
        "        v2 = np.linalg.norm(landmarks[158] - landmarks[153])\n",
        "        v3 = np.linalg.norm(landmarks[160] - landmarks[144])\n",
        "    else:\n",
        "        outer = landmarks[362]\n",
        "        inner = landmarks[263]\n",
        "        v1 = np.linalg.norm(landmarks[386] - landmarks[374])\n",
        "        v2 = np.linalg.norm(landmarks[385] - landmarks[373])\n",
        "        v3 = np.linalg.norm(landmarks[387] - landmarks[380])\n",
        "\n",
        "    horizontal = np.linalg.norm(outer - inner)\n",
        "    avg_vertical = (v1 + v2 + v3) / 3.0\n",
        "    return avg_vertical / horizontal\n",
        "\n",
        "def calculate_adaptive_ear_thresholds(all_ear_values):\n",
        "    \"\"\"ì˜ìƒë³„ ì ì‘í˜• EAR ì„ê³„ê°’ ê³„ì‚° - 50% ê¸°ì¤€ ì ìš©\"\"\"\n",
        "    if len(all_ear_values) < 5:\n",
        "        return {\n",
        "            'high_threshold': 0.23,\n",
        "            'medium_threshold': 0.18,\n",
        "            'low_threshold': 0.15,\n",
        "            'min_threshold': 0.12\n",
        "        }\n",
        "\n",
        "    ear_array = np.array(all_ear_values)\n",
        "\n",
        "    # 50% ê¸°ì¤€ìœ¼ë¡œ ì„ê³„ê°’ ì„¤ì •\n",
        "    high_threshold = np.percentile(ear_array, 100 - EAR_PERCENTILE_HIGH)  # ìƒìœ„ 50%\n",
        "    medium_threshold = np.percentile(ear_array, 100 - EAR_PERCENTILE_HIGH * 1.2)  # ìƒìœ„ 60%\n",
        "    low_threshold = np.percentile(ear_array, 100 - EAR_PERCENTILE_HIGH * 1.5)  # ìƒìœ„ 75%\n",
        "    min_threshold = np.percentile(ear_array, EAR_PERCENTILE_LOW)  # í•˜ìœ„ 10%\n",
        "\n",
        "    # ìµœì†Œê°’ ë³´ì¥\n",
        "    high_threshold = max(high_threshold, 0.16)  # ë” ê´€ëŒ€í•˜ê²Œ\n",
        "    medium_threshold = max(medium_threshold, 0.13)\n",
        "    low_threshold = max(low_threshold, 0.10)\n",
        "    min_threshold = max(min_threshold, 0.07)\n",
        "\n",
        "    return {\n",
        "        'high_threshold': high_threshold,\n",
        "        'medium_threshold': medium_threshold,\n",
        "        'low_threshold': low_threshold,\n",
        "        'min_threshold': min_threshold,\n",
        "        'ear_stats': {\n",
        "            'mean': np.mean(ear_array),\n",
        "            'std': np.std(ear_array),\n",
        "            'min': np.min(ear_array),\n",
        "            'max': np.max(ear_array),\n",
        "            'count': len(ear_array)\n",
        "        }\n",
        "    }\n",
        "\n",
        "def is_eye_open_adaptive(landmarks, thresholds, pitch_angle=0):\n",
        "    \"\"\"ì ì‘í˜• EAR ê¸°ë°˜ ëˆˆëœ¸ íŒì • - í”¼ì¹˜ ì¡°ì • ìµœì†Œí™”\"\"\"\n",
        "\n",
        "    left_eye_basic = landmarks[CORE_LEFT_EYE]\n",
        "    right_eye_basic = landmarks[CORE_RIGHT_EYE]\n",
        "\n",
        "    basic_left_ear = eye_aspect_ratio(left_eye_basic)\n",
        "    basic_right_ear = eye_aspect_ratio(right_eye_basic)\n",
        "    basic_avg_ear = (basic_left_ear + basic_right_ear) / 2.0\n",
        "\n",
        "    enhanced_left_ear = enhanced_eye_aspect_ratio(landmarks, True)\n",
        "    enhanced_right_ear = enhanced_eye_aspect_ratio(landmarks, False)\n",
        "    enhanced_avg_ear = (enhanced_left_ear + enhanced_right_ear) / 2.0\n",
        "\n",
        "    ear_difference = abs(basic_left_ear - basic_right_ear)\n",
        "\n",
        "    # í”¼ì¹˜ ì¡°ì • ìµœì†Œí™” (ê´€ëŒ€í•˜ê²Œ í•˜ì§€ ì•ŠìŒ)\n",
        "    pitch_factor = 1.0\n",
        "    if pitch_angle > 20:  # 20ë„ ì´ìƒì—ì„œë§Œ ìµœì†Œ ì¡°ì •\n",
        "        pitch_factor = max(0.95, 1.0 - (pitch_angle - 20) * 0.005)  # ìµœëŒ€ 5%ë§Œ ì™„í™”\n",
        "\n",
        "    # ì ì‘í˜• ì„ê³„ê°’ ì ìš©\n",
        "    adj_high = thresholds['high_threshold'] * pitch_factor\n",
        "    adj_medium = thresholds['medium_threshold'] * pitch_factor\n",
        "    adj_low = thresholds['low_threshold'] * pitch_factor\n",
        "    adj_min = thresholds['min_threshold'] * pitch_factor\n",
        "\n",
        "    # 4ë‹¨ê³„ ëˆˆëœ¸ íŒì •\n",
        "    level_1 = (basic_avg_ear > adj_high and\n",
        "               enhanced_avg_ear > adj_high * 0.9 and\n",
        "               basic_left_ear > adj_medium and\n",
        "               basic_right_ear > adj_medium and\n",
        "               ear_difference < 0.08)\n",
        "\n",
        "    level_2 = (basic_avg_ear > adj_medium and\n",
        "               enhanced_avg_ear > adj_medium * 0.8 and\n",
        "               basic_left_ear > adj_low and\n",
        "               basic_right_ear > adj_low and\n",
        "               ear_difference < 0.12)\n",
        "\n",
        "    level_3 = (basic_avg_ear > adj_low and\n",
        "               basic_left_ear > adj_min and\n",
        "               basic_right_ear > adj_min and\n",
        "               ear_difference < 0.15)\n",
        "\n",
        "    level_4 = (basic_avg_ear > adj_min and\n",
        "               basic_left_ear > adj_min * 0.8 and\n",
        "               basic_right_ear > adj_min * 0.8)\n",
        "\n",
        "    # ë ˆë²¨ë³„ ì ìˆ˜ ë¶€ì—¬\n",
        "    if level_1:\n",
        "        eye_level = 4\n",
        "    elif level_2:\n",
        "        eye_level = 3\n",
        "    elif level_3:\n",
        "        eye_level = 2\n",
        "    elif level_4:\n",
        "        eye_level = 1\n",
        "    else:\n",
        "        eye_level = 0\n",
        "\n",
        "    return eye_level, {\n",
        "        'basic_ear': basic_avg_ear,\n",
        "        'enhanced_ear': enhanced_avg_ear,\n",
        "        'left_ear': basic_left_ear,\n",
        "        'right_ear': basic_right_ear,\n",
        "        'ear_diff': ear_difference,\n",
        "        'eye_level': eye_level,\n",
        "        'pitch_factor': pitch_factor,\n",
        "        'thresholds_used': {\n",
        "            'high': adj_high,\n",
        "            'medium': adj_medium,\n",
        "            'low': adj_low,\n",
        "            'min': adj_min\n",
        "        }\n",
        "    }\n",
        "\n",
        "def frontal_score_strict_pitch(c):\n",
        "    \"\"\"í”¼ì¹˜ ì œí•œ ê°•í™”ëœ ì •ë©´ì„± í‰ê°€ í•¨ìˆ˜\"\"\"\n",
        "\n",
        "    yaw_angle = abs(c['yaw'])\n",
        "    pitch_angle = c['pitch']\n",
        "\n",
        "    # ì—„ê²©í•œ ê°ë„ ì œí•œ (í”¼ì¹˜ ê´€ëŒ€í•˜ê²Œ í•˜ì§€ ì•ŠìŒ)\n",
        "    if yaw_angle > YAW_T:  # 25ë„\n",
        "        return -1000\n",
        "    if abs(pitch_angle) > PITCH_T:  # Â±25ë„ (ì—„ê²©)\n",
        "        return -1000\n",
        "\n",
        "    # ê¸°ë³¸ í˜ë„í‹° (í”¼ì¹˜ì— ë” í° ê°€ì¤‘ì¹˜)\n",
        "    yaw_penalty = yaw_angle * 0.8\n",
        "    pitch_penalty = abs(pitch_angle) * 1.2  # í”¼ì¹˜ í˜ë„í‹° ì¦ê°€\n",
        "\n",
        "    # ê³ ê°œ ìˆ™ì„ ë³´ë„ˆìŠ¤ ìµœì†Œí™”\n",
        "    head_down_bonus = 0\n",
        "    if -15 <= pitch_angle <= -5:  # ì•„ì£¼ ì œí•œì ì¸ ë²”ìœ„ì—ì„œë§Œ\n",
        "        head_down_bonus = HEAD_DOWN_BONUS * 0.5  # ë³´ë„ˆìŠ¤ë„ ì ˆë°˜ìœ¼ë¡œ\n",
        "\n",
        "    # ëˆˆëœ¸ ë ˆë²¨ ë³´ë„ˆìŠ¤\n",
        "    eye_level = c.get('eye_level', 0)\n",
        "    eye_bonus = eye_level * 12  # ëˆˆëœ¸ì´ ë” ì¤‘ìš”\n",
        "\n",
        "    bonus = eye_bonus + (head_down_bonus if pitch_angle < 0 else 0)\n",
        "\n",
        "    return -(0.5 * yaw_penalty + 0.5 * pitch_penalty) + bonus\n",
        "\n",
        "def calculate_final_quality_score(pitch, yaw, eye_level, ear_details):\n",
        "    \"\"\"ìµœì¢… í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° - í”¼ì¹˜ í˜ë„í‹° ê°•í™”\"\"\"\n",
        "\n",
        "    # ê°ë„ ì ìˆ˜ (í”¼ì¹˜ì— ë” í° í˜ë„í‹°)\n",
        "    yaw_score = max(0, 100 - abs(yaw) * 2.0)\n",
        "    pitch_score = max(0, 100 - abs(pitch) * 2.5)  # í”¼ì¹˜ í˜ë„í‹° ì¦ê°€\n",
        "    angle_score = (yaw_score + pitch_score) / 2\n",
        "\n",
        "    # ëˆˆëœ¸ ë ˆë²¨ ì ìˆ˜ (50% ê¸°ì¤€ì´ë¯€ë¡œ ë” ê´€ëŒ€)\n",
        "    eye_score = eye_level * 25\n",
        "\n",
        "    # EAR í’ˆì§ˆ ì ìˆ˜\n",
        "    ear_quality = min(100, ear_details['basic_ear'] * 300)\n",
        "\n",
        "    # ì¢…í•© ì ìˆ˜ (ëˆˆëœ¸ ë¹„ì¤‘ ì¦ê°€)\n",
        "    total_score = (\n",
        "        angle_score * 0.3 +    # ê°ë„ 30%\n",
        "        eye_score * 0.5 +      # ëˆˆëœ¸ 50% (ì¦ê°€)\n",
        "        ear_quality * 0.2      # EAR í’ˆì§ˆ 20%\n",
        "    )\n",
        "\n",
        "    return total_score\n",
        "\n",
        "# ê²½ë¡œ ì„¤ì •\n",
        "INPUT_ROOT = \"/content/extract_frames\"\n",
        "OUTPUT_ROOT = \"/content/alignmented_frame\"\n",
        "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
        "\n",
        "file_ext = \".jpg\"\n",
        "video_dirs = [d for d in os.listdir(INPUT_ROOT) if os.path.isdir(os.path.join(INPUT_ROOT, d))]\n",
        "missing_videos = []\n",
        "detailed_log = []\n",
        "best_images = []\n",
        "best_names = []\n",
        "\n",
        "print(f\"ğŸ“¦ ì´ {len(video_dirs)}ê°œ ì˜ìƒ ì²˜ë¦¬ ì‹œì‘\")\n",
        "print(f\"ğŸ¯ ëˆˆëœ¸ ê¸°ì¤€: ìƒìœ„ {EAR_PERCENTILE_HIGH}% (50% ê¸°ì¤€ìœ¼ë¡œ ì™„í™”)\")\n",
        "print(f\"ğŸ“ ê°ë„ ì œí•œ: Yaw Â±{YAW_T}Â°, Pitch Â±{PITCH_T}Â° (í”¼ì¹˜ ì œí•œ ê°•í™”)\")\n",
        "print(f\"ğŸ ê³ ê°œ ìˆ™ì„ ë³´ë„ˆìŠ¤: {HEAD_DOWN_BONUS}ì  (ìµœì†Œí™”)\")\n",
        "\n",
        "for video_name in tqdm(video_dirs, desc=\"ğŸ¯ Strict pitch + 50% eye threshold\"):\n",
        "    input_dir = os.path.join(INPUT_ROOT, video_name)\n",
        "\n",
        "    # 1ë‹¨ê³„: ëª¨ë“  í”„ë ˆì„ì˜ EAR ê°’ ìˆ˜ì§‘\n",
        "    all_ear_values = []\n",
        "    frame_data = []\n",
        "\n",
        "    for f in sorted(os.listdir(input_dir)):\n",
        "        if not f.lower().endswith(file_ext):\n",
        "            continue\n",
        "\n",
        "        full_path = os.path.join(input_dir, f)\n",
        "        img = cv2.imread(full_path)\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        lm = get_mediapipe_landmarks(img)\n",
        "        if lm is None:\n",
        "            continue\n",
        "\n",
        "        rv = estimate_pose_mediapipe(lm, img.shape)\n",
        "        if rv is None:\n",
        "            continue\n",
        "\n",
        "        pitch, yaw, _ = rotation_vector_to_euler(rv)\n",
        "\n",
        "        # ì—„ê²©í•œ ê°ë„ ì œí•œ\n",
        "        if abs(yaw) > YAW_T * 1.5 or abs(pitch) > PITCH_T * 1.5:\n",
        "            continue\n",
        "\n",
        "        # EAR ê³„ì‚°\n",
        "        left_eye = lm[CORE_LEFT_EYE]\n",
        "        right_eye = lm[CORE_RIGHT_EYE]\n",
        "        left_ear = eye_aspect_ratio(left_eye)\n",
        "        right_ear = eye_aspect_ratio(right_eye)\n",
        "        avg_ear = (left_ear + right_ear) / 2.0\n",
        "\n",
        "        all_ear_values.append(avg_ear)\n",
        "\n",
        "        # í”„ë ˆì„ ë°ì´í„° ì €ì¥\n",
        "        x1, y1 = lm[:,0].min(), lm[:,1].min()\n",
        "        x2, y2 = lm[:,0].max(), lm[:,1].max()\n",
        "        face_area = (x2 - x1) * (y2 - y1)\n",
        "        cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n",
        "\n",
        "        frame_data.append({\n",
        "            'img': img,\n",
        "            'filename': f,\n",
        "            'landmarks': lm,\n",
        "            'pitch': pitch,\n",
        "            'yaw': yaw,\n",
        "            'avg_ear': avg_ear,\n",
        "            'cx': cx,\n",
        "            'cy': cy,\n",
        "            'face_area': face_area\n",
        "        })\n",
        "\n",
        "    if not all_ear_values:\n",
        "        missing_videos.append(video_name)\n",
        "        detailed_log.append({\n",
        "            'video_name': video_name,\n",
        "            'total_frames': 0,\n",
        "            'selected': False,\n",
        "            'selection_level': 0,  # KeyError ë°©ì§€ë¥¼ ìœ„í•´ ì¶”ê°€\n",
        "            'reason': 'No valid frames found'\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    # 2ë‹¨ê³„: ì ì‘í˜• ì„ê³„ê°’ ê³„ì‚° (50% ê¸°ì¤€)\n",
        "    thresholds = calculate_adaptive_ear_thresholds(all_ear_values)\n",
        "\n",
        "    # 3ë‹¨ê³„: 4ë‹¨ê³„ í›„ë³´ ë¶„ë¥˜\n",
        "    level_4_candidates = []\n",
        "    level_3_candidates = []\n",
        "    level_2_candidates = []\n",
        "    level_1_candidates = []\n",
        "\n",
        "    for frame in frame_data:\n",
        "        # ì—„ê²©í•œ í”¼ì¹˜ ì œí•œ ì ìš©\n",
        "        if abs(frame['pitch']) > PITCH_T:\n",
        "            continue\n",
        "\n",
        "        eye_level, eye_details = is_eye_open_adaptive(\n",
        "            frame['landmarks'], thresholds, abs(frame['pitch'])\n",
        "        )\n",
        "\n",
        "        if eye_level == 0:\n",
        "            continue\n",
        "\n",
        "        candidate = {\n",
        "            'img': frame['img'],\n",
        "            'filename': frame['filename'],\n",
        "            'pitch': frame['pitch'],\n",
        "            'yaw': frame['yaw'],\n",
        "            'cx': frame['cx'],\n",
        "            'cy': frame['cy'],\n",
        "            'face_area': frame['face_area'],\n",
        "            'w': frame['img'].shape[1],\n",
        "            'h': frame['img'].shape[0],\n",
        "            'eye_level': eye_level,\n",
        "            'eye_details': eye_details,\n",
        "            'quality_score': calculate_final_quality_score(\n",
        "                frame['pitch'], frame['yaw'], eye_level, eye_details\n",
        "            )\n",
        "        }\n",
        "\n",
        "        if eye_level == 4:\n",
        "            level_4_candidates.append(candidate)\n",
        "        elif eye_level == 3:\n",
        "            level_3_candidates.append(candidate)\n",
        "        elif eye_level == 2:\n",
        "            level_2_candidates.append(candidate)\n",
        "        else:\n",
        "            level_1_candidates.append(candidate)\n",
        "\n",
        "    # 4ë‹¨ê³„: ìµœì  í”„ë ˆì„ ì„ íƒ\n",
        "    best_img = None\n",
        "    best_filename = None\n",
        "    selection_reason = \"No suitable frames\"\n",
        "    selection_level = 0\n",
        "\n",
        "    if level_4_candidates:\n",
        "        best = max(level_4_candidates, key=frontal_score_strict_pitch)\n",
        "        best_img = best['img']\n",
        "        best_filename = best['filename']\n",
        "        selection_reason = f\"Level 4: Selected from {len(level_4_candidates)} highest quality candidates\"\n",
        "        selection_level = 4\n",
        "\n",
        "    elif level_3_candidates:\n",
        "        best = max(level_3_candidates, key=frontal_score_strict_pitch)\n",
        "        best_img = best['img']\n",
        "        best_filename = best['filename']\n",
        "        selection_reason = f\"Level 3: Selected from {len(level_3_candidates)} high quality candidates\"\n",
        "        selection_level = 3\n",
        "\n",
        "    elif level_2_candidates:\n",
        "        best = max(level_2_candidates, key=frontal_score_strict_pitch)\n",
        "        best_img = best['img']\n",
        "        best_filename = best['filename']\n",
        "        selection_reason = f\"Level 2: Selected from {len(level_2_candidates)} medium quality candidates\"\n",
        "        selection_level = 2\n",
        "\n",
        "    elif level_1_candidates:\n",
        "        best = max(level_1_candidates, key=lambda x: x['quality_score'])\n",
        "        best_img = best['img']\n",
        "        best_filename = best['filename']\n",
        "        selection_reason = f\"Level 1: Selected from {len(level_1_candidates)} minimum quality candidates\"\n",
        "        selection_level = 1\n",
        "\n",
        "    elif frame_data:\n",
        "        # ìµœí›„ì˜ ìˆ˜ë‹¨: ê°€ì¥ í’ˆì§ˆ ì¢‹ì€ í”„ë ˆì„ ë¬´ì¡°ê±´ ì„ íƒ\n",
        "        best_frame = max(frame_data, key=lambda x: x['avg_ear'])\n",
        "        best_img = best_frame['img']\n",
        "        best_filename = best_frame['filename']\n",
        "        selection_reason = f\"Emergency: Selected best EAR frame ({best_frame['avg_ear']:.3f})\"\n",
        "        selection_level = 0\n",
        "\n",
        "    # KeyError ë°©ì§€ë¥¼ ìœ„í•œ ì•ˆì „í•œ ë¡œê·¸ ê¸°ë¡\n",
        "    detailed_log.append({\n",
        "        'video_name': video_name,\n",
        "        'total_frames': len(frame_data),\n",
        "        'level_4_candidates': len(level_4_candidates),\n",
        "        'level_3_candidates': len(level_3_candidates),\n",
        "        'level_2_candidates': len(level_2_candidates),\n",
        "        'level_1_candidates': len(level_1_candidates),\n",
        "        'selected': best_filename is not None,\n",
        "        'selection_level': selection_level,  # í•­ìƒ í¬í•¨\n",
        "        'best_filename': best_filename,\n",
        "        'reason': selection_reason,\n",
        "        'ear_thresholds': thresholds,\n",
        "        'quality_score': best.get('quality_score', 0) if 'best' in locals() else 0\n",
        "    })\n",
        "\n",
        "    # í”„ë ˆì„ ì €ì¥\n",
        "    if best_img is not None:\n",
        "        save_path = os.path.join(OUTPUT_ROOT, f\"{video_name}.jpg\")\n",
        "        cv2.imwrite(save_path, best_img)\n",
        "\n",
        "        if selection_level <= 1:\n",
        "            print(f\"âš ï¸  {video_name}: Low quality selection (Level {selection_level})\")\n",
        "    else:\n",
        "        missing_videos.append(video_name)\n",
        "\n",
        "# ê²°ê³¼ ì €ì¥\n",
        "if missing_videos:\n",
        "    df_missing = pd.DataFrame(missing_videos, columns=[\"video_name\"])\n",
        "    df_missing.to_csv(\"no_frame_found.csv\", index=False)\n",
        "    print(f\"â— {len(missing_videos)}ê°œ ì˜ìƒì—ì„œ í”„ë ˆì„ì„ ì°¾ì§€ ëª»í•¨\")\n",
        "\n",
        "df_log = pd.DataFrame(detailed_log)\n",
        "df_log.to_csv(\"strict_pitch_50percent_eye_log.csv\", index=False)\n",
        "print(f\"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼ ì €ì¥: strict_pitch_50percent_eye_log.csv\")\n",
        "\n",
        "# KeyError ë°©ì§€ëœ ì•ˆì „í•œ í†µê³„ ì¶œë ¥\n",
        "success_rate = (len(video_dirs) - len(missing_videos)) / len(video_dirs) * 100 if video_dirs else 0\n",
        "\n",
        "# .get() ë©”ì„œë“œ ì‚¬ìš©ìœ¼ë¡œ KeyError ë°©ì§€\n",
        "level_4_usage = sum(1 for log in detailed_log if log.get('selection_level', 0) == 4)\n",
        "level_3_usage = sum(1 for log in detailed_log if log.get('selection_level', 0) == 3)\n",
        "level_2_usage = sum(1 for log in detailed_log if log.get('selection_level', 0) == 2)\n",
        "level_1_usage = sum(1 for log in detailed_log if log.get('selection_level', 0) == 1)\n",
        "emergency_usage = sum(1 for log in detailed_log if log.get('selection_level', 0) == 0)\n",
        "\n",
        "print(f\"âœ… ì„±ê³µë¥ : {success_rate:.1f}% ({len(video_dirs) - len(missing_videos)}/{len(video_dirs)})\")\n",
        "print(f\"ğŸ† Level 4 (ìµœê³ í’ˆì§ˆ): {level_4_usage}ê°œ\")\n",
        "print(f\"ğŸ¥ˆ Level 3 (ê³ í’ˆì§ˆ): {level_3_usage}ê°œ\")\n",
        "print(f\"ğŸ¥‰ Level 2 (ì¤‘í’ˆì§ˆ): {level_2_usage}ê°œ\")\n",
        "print(f\"ğŸ“‰ Level 1 (ìµœì†Œí’ˆì§ˆ): {level_1_usage}ê°œ\")\n",
        "print(f\"ğŸš¨ Emergency (ê°•ì œì„ íƒ): {emergency_usage}ê°œ\")\n",
        "\n",
        "print(f\"\\nğŸ“ˆ í’ˆì§ˆ ë¶„í¬:\")\n",
        "print(f\"   - ê³ í’ˆì§ˆ ì´ìƒ (Level 3+): {(level_4_usage + level_3_usage) / len(video_dirs) * 100:.1f}%\")\n",
        "print(f\"   - ì¤‘í’ˆì§ˆ ì´ìƒ (Level 2+): {(level_4_usage + level_3_usage + level_2_usage) / len(video_dirs) * 100:.1f}%\")\n",
        "print(f\"   - ìµœì†Œí’ˆì§ˆ ì´ìƒ (Level 1+): {(len(video_dirs) - emergency_usage) / len(video_dirs) * 100:.1f}%\")\n",
        "\n",
        "print(f\"\\nğŸ¯ ì„¤ì • ìš”ì•½:\")\n",
        "print(f\"   - í”¼ì¹˜ ì œí•œ: Â±{PITCH_T}Â° (ì—„ê²©)\")\n",
        "print(f\"   - ëˆˆëœ¸ ê¸°ì¤€: ìƒìœ„ {EAR_PERCENTILE_HIGH}% (ê´€ëŒ€)\")\n",
        "print(f\"   - ê³ ê°œ ìˆ™ì„ ë³´ë„ˆìŠ¤: {HEAD_DOWN_BONUS}ì  (ìµœì†Œ)\")\n"
      ],
      "metadata": {
        "id": "jlELfbUkEFea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "print(numpy.__version__)"
      ],
      "metadata": {
        "id": "H_23qUpRqSYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# image crop"
      ],
      "metadata": {
        "id": "22A2atJxxcBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/stylegan3-editing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W19HlMQIxcMs",
        "outputId": "743b315b-76f8-4aeb-a9c8-386649bee871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/stylegan3-editing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "base_raw_root = \"/content/alignmented_frame\"\n",
        "aligned_root = f\"{base_raw_root}_aligned\"\n",
        "cropped_root = f\"{base_raw_root}_croped\"\n",
        "transform_root = f\"{base_raw_root}_transforms\"\n",
        "\n",
        "print(\"ğŸš€ Aligning all images...\")\n",
        "!python /content/stylegan3-editing/prepare_data/preparing_faces_parallel.py \\\n",
        "    --mode align \\\n",
        "    --root_path \"{base_raw_root}\"\n",
        "\n",
        "time.sleep(3)\n",
        "\n",
        "print(\"ğŸ” Cropping all images...\")\n",
        "!python /content/stylegan3-editing/prepare_data/preparing_faces_parallel.py \\\n",
        "    --mode crop \\\n",
        "    --root_path \"{base_raw_root}\" \\\n",
        "    --random_shift 0.05\n",
        "\n",
        "time.sleep(3)\n",
        "\n",
        "print(\"ğŸ” Computing transforms for all images...\")\n",
        "!python /content/stylegan3-editing/prepare_data/compute_landmarks_transforms.py \\\n",
        "    --raw_root \"{base_raw_root}\" \\\n",
        "    --aligned_root \"{aligned_root}\" \\\n",
        "    --cropped_root \"{cropped_root}\" \\\n",
        "    --output_root \"{transform_root}\"\n",
        "\n",
        "print(\"âœ… All preprocessing completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Kzg1k-FxgA2",
        "outputId": "3915a24c-6f62-4acb-ca45-171d941af60e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Aligning all images...\n",
            "1\n",
            "Running on 16 paths\n",
            "Here we goooo\n",
            "\tForkPoolWorker-1 is starting to extract on #16 images\n",
            "\tDone!\n",
            "Mischief managed in -9.975287914276123s\n",
            "ğŸ” Cropping all images...\n",
            "1\n",
            "Running on 16 paths\n",
            "Here we goooo\n",
            "\tForkPoolWorker-1 is starting to extract on #16 images\n",
            "\tDone!\n",
            "Mischief managed in -8.99843692779541s\n",
            "ğŸ” Computing transforms for all images...\n",
            "Computing landmarks transforms...\n",
            "  6% 1/16 [00:02<00:33,  2.26s/it]Failed aligning image: /content/alignmented_frame_aligned/0122.png. Got exception: cannot access local variable 'shape' where it is not associated with a value\n",
            "Failed on: /content/alignmented_frame_croped/0122.png\n",
            " 19% 3/16 [00:05<00:26,  2.01s/it]Failed aligning image: /content/alignmented_frame_aligned/0022.png. Got exception: cannot access local variable 'shape' where it is not associated with a value\n",
            "Failed on: /content/alignmented_frame_croped/0022.png\n",
            " 62% 10/16 [00:21<00:14,  2.43s/it]Failed aligning image: /content/alignmented_frame_aligned/0480.png. Got exception: cannot access local variable 'shape' where it is not associated with a value\n",
            "Failed on: /content/alignmented_frame_croped/0480.png\n",
            " 69% 11/16 [00:22<00:09,  1.90s/it]Failed aligning image: /content/alignmented_frame_aligned/0552.png. Got exception: cannot access local variable 'shape' where it is not associated with a value\n",
            "Failed on: /content/alignmented_frame_croped/0552.png\n",
            " 94% 15/16 [00:29<00:01,  1.97s/it]Failed aligning image: /content/alignmented_frame_aligned/0483.png. Got exception: cannot access local variable 'shape' where it is not associated with a value\n",
            "Failed on: /content/alignmented_frame_croped/0483.png\n",
            "100% 16/16 [00:30<00:00,  1.89s/it]\n",
            "âœ… All preprocessing completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# ë””ë ‰í† ë¦¬ ì„¤ì •\n",
        "input_root = \"/content/alignmented_frame_croped\"\n",
        "transforms_root = \"/content/alignmented_frame_transforms/landmarks_transforms.npy\"\n",
        "output_root = \"/content/experiments/restyle_e4e_sg3\"\n",
        "ckpt_path = \"/content/pretrained_models/restyle_e4e_sg3.pt\"\n",
        "script_path = \"/content/stylegan3-editing/inversion/scripts/inference_iterative.py\"\n",
        "\n",
        "# output ë””ë ‰í† ë¦¬ ìƒì„±\n",
        "os.makedirs(output_root, exist_ok=True)\n",
        "\n",
        "print(\"ğŸš€ Inverting video\")\n",
        "\n",
        "!python {script_path} \\\n",
        "    --output_path \"{output_root}\" \\\n",
        "    --checkpoint_path \"{ckpt_path}\" \\\n",
        "    --data_path \"{input_root}\" \\\n",
        "    --test_batch_size 4 \\\n",
        "    --test_workers 4 \\\n",
        "    --n_iters_per_batch 3 \\\n",
        "    --landmarks_transforms_path \"{transforms_root}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONdozUaBz7ja",
        "outputId": "769b93df-63d8-40da-f314-2277989bc1ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Inverting video\n",
            "Loading ReStyle e4e from checkpoint: /content/pretrained_models/restyle_e4e_sg3.pt\n",
            "Loading StyleGAN3 generator from path: None\n",
            "Done!\n",
            "Model successfully loaded!\n",
            "Loading dataset for ffhq_encode\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Setting up PyTorch plugin \"filtered_lrelu_plugin\"... Done.\n",
            "  0% 0/3 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "100% 3/3 [00:55<00:00, 18.44s/it]\n",
            "Runtime 5.4759+-0.6984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# save CSV"
      ],
      "metadata": {
        "id": "BAaQSZUwoGsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Load latent dictionary\n",
        "latent_path = \"/content/experiments/restyle_e4e_sg3/latents.npy\"\n",
        "latent_dict = np.load(latent_path, allow_pickle=True).item()\n",
        "\n",
        "# 2. ì •ë ¬ëœ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ í™•ë³´\n",
        "filenames = sorted(latent_dict.keys())\n",
        "\n",
        "# 3. ê° latentì—ì„œ ë§ˆì§€ë§‰ step â†’ í‰ê·  â†’ (512,)\n",
        "latents = []\n",
        "for key in filenames:\n",
        "    latent = latent_dict[key][-1]  # ë§ˆì§€ë§‰ step (18, 512)\n",
        "    mean_latent = latent.mean(axis=0).astype('float32')  # (512,)\n",
        "    latents.append(mean_latent)\n",
        "\n",
        "latents = np.stack(latents)  # shape: (N, 512)\n",
        "\n",
        "# 4. cosine similarity matrix\n",
        "sim_matrix = cosine_similarity(latents)  # shape: (N, N)\n",
        "\n",
        "# 5. ê° query íŒŒì¼ë§ˆë‹¤ top-3 ìœ ì‚¬í•œ match + score ì €ì¥\n",
        "rows = []\n",
        "\n",
        "for i in range(len(filenames)):\n",
        "    sims = sim_matrix[i].copy()\n",
        "    sims[i] = -np.inf  # ìê¸° ìì‹  ì œì™¸\n",
        "    top3_idx = np.argsort(sims)[::-1][:3]\n",
        "    row = {\n",
        "        \"query\": filenames[i],\n",
        "        \"top1\": filenames[top3_idx[0]],\n",
        "        \"top2\": filenames[top3_idx[1]],\n",
        "        \"top3\": filenames[top3_idx[2]],\n",
        "        \"top1val\": round(float(sims[top3_idx[0]]), 6),\n",
        "        \"top2val\": round(float(sims[top3_idx[1]]), 6),\n",
        "        \"top3val\": round(float(sims[top3_idx[2]]), 6),\n",
        "    }\n",
        "    rows.append(row)\n",
        "\n",
        "# 6. Save to CSV\n",
        "df = pd.DataFrame(rows)\n",
        "df.to_csv(\"video_similarity_top3_compact.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Saved to video_similarity_top3_compact.csv\")\n"
      ],
      "metadata": {
        "id": "WjMFJ5i2oE78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64b07280-dbda-49f0-fda8-897a7f6ea90a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Saved to video_similarity_top3_compact.csv\n"
          ]
        }
      ]
    }
  ]
}