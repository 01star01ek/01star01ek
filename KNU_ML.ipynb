{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOaJpGZHeo+qQtLo3vZoxLb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/01star01ek/01star01ek/blob/main/KNU_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZGeKK10ntpK",
        "outputId": "d7c9cdeb-7e6e-4e85-a344-546aeb38bb52"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data.zip\n",
            "  inflating: sample_submission.csv   \n",
            "   creating: te-re-g/\n",
            "  inflating: te-re-g/000.jpg         \n",
            "  inflating: te-re-g/001.jpg         \n",
            "  inflating: te-re-g/002.jpg         \n",
            "  inflating: te-re-g/003.jpg         \n",
            "  inflating: te-re-g/004.jpg         \n",
            "  inflating: te-re-g/005.jpg         \n",
            "  inflating: te-re-g/006.jpg         \n",
            "  inflating: te-re-g/007.jpg         \n",
            "  inflating: te-re-g/008.jpg         \n",
            "  inflating: te-re-g/009.jpg         \n",
            "  inflating: te-re-g/010.jpg         \n",
            "  inflating: te-re-g/011.jpg         \n",
            "  inflating: te-re-g/012.jpg         \n",
            "  inflating: te-re-g/013.jpg         \n",
            "  inflating: te-re-g/014.jpg         \n",
            "  inflating: te-re-g/015.jpg         \n",
            "  inflating: te-re-g/016.jpg         \n",
            "  inflating: te-re-g/017.jpg         \n",
            "  inflating: te-re-g/018.jpg         \n",
            "  inflating: te-re-g/019.jpg         \n",
            "  inflating: te-re-g/020.jpg         \n",
            "  inflating: te-re-g/021.jpg         \n",
            "  inflating: te-re-g/022.jpg         \n",
            "  inflating: te-re-g/023.jpg         \n",
            "  inflating: te-re-g/024.jpg         \n",
            "  inflating: te-re-g/025.jpg         \n",
            "  inflating: te-re-g/026.jpg         \n",
            "  inflating: te-re-g/027.jpg         \n",
            "  inflating: te-re-g/028.jpg         \n",
            "  inflating: te-re-g/029.jpg         \n",
            "  inflating: te-re-g/030.jpg         \n",
            "  inflating: te-re-g/031.jpg         \n",
            "  inflating: te-re-g/032.jpg         \n",
            "  inflating: te-re-g/033.jpg         \n",
            "  inflating: te-re-g/034.jpg         \n",
            "  inflating: te-re-g/035.jpg         \n",
            "  inflating: te-re-g/036.jpg         \n",
            "  inflating: te-re-g/037.jpg         \n",
            "  inflating: te-re-g/038.jpg         \n",
            "  inflating: te-re-g/039.jpg         \n",
            "  inflating: te-re-g/040.jpg         \n",
            "  inflating: te-re-g/041.jpg         \n",
            "  inflating: te-re-g/042.jpg         \n",
            "  inflating: te-re-g/043.jpg         \n",
            "  inflating: te-re-g/044.jpg         \n",
            "  inflating: te-re-g/045.jpg         \n",
            "  inflating: te-re-g/046.jpg         \n",
            "  inflating: te-re-g/047.jpg         \n",
            "  inflating: te-re-g/048.jpg         \n",
            "  inflating: te-re-g/049.jpg         \n",
            "  inflating: te-re-g/050.jpg         \n",
            "  inflating: te-re-g/051.jpg         \n",
            "  inflating: te-re-g/052.jpg         \n",
            "  inflating: te-re-g/053.jpg         \n",
            "  inflating: te-re-g/054.jpg         \n",
            "  inflating: te-re-g/055.jpg         \n",
            "  inflating: te-re-g/056.jpg         \n",
            "  inflating: te-re-g/057.jpg         \n",
            "  inflating: te-re-g/058.jpg         \n",
            "  inflating: te-re-g/059.jpg         \n",
            "  inflating: te-re-g/060.jpg         \n",
            "  inflating: te-re-g/061.jpg         \n",
            "  inflating: te-re-g/062.jpg         \n",
            "  inflating: te-re-g/063.jpg         \n",
            "  inflating: te-re-g/064.jpg         \n",
            "  inflating: te-re-g/065.jpg         \n",
            "  inflating: te-re-g/066.jpg         \n",
            "  inflating: te-re-g/067.jpg         \n",
            "  inflating: te-re-g/068.jpg         \n",
            "  inflating: te-re-g/069.jpg         \n",
            "  inflating: te-re-g/070.jpg         \n",
            "  inflating: te-re-g/071.jpg         \n",
            "  inflating: te-re-g/072.jpg         \n",
            "  inflating: te-re-g/073.jpg         \n",
            "  inflating: te-re-g/074.jpg         \n",
            "  inflating: te-re-g/075.jpg         \n",
            "  inflating: te-re-g/076.jpg         \n",
            "  inflating: te-re-g/077.jpg         \n",
            "  inflating: te-re-g/078.jpg         \n",
            "  inflating: te-re-g/079.jpg         \n",
            "  inflating: te-re-g/080.jpg         \n",
            "  inflating: te-re-g/081.jpg         \n",
            "  inflating: te-re-g/082.jpg         \n",
            "  inflating: te-re-g/083.jpg         \n",
            "  inflating: te-re-g/084.jpg         \n",
            "  inflating: te-re-g/085.jpg         \n",
            "  inflating: te-re-g/086.jpg         \n",
            "  inflating: te-re-g/087.jpg         \n",
            "  inflating: te-re-g/088.jpg         \n",
            "  inflating: te-re-g/089.jpg         \n",
            "  inflating: te-re-g/090.jpg         \n",
            "  inflating: te-re-g/091.jpg         \n",
            "  inflating: te-re-g/092.jpg         \n",
            "  inflating: te-re-g/093.jpg         \n",
            "  inflating: te-re-g/094.jpg         \n",
            "  inflating: te-re-g/095.jpg         \n",
            "  inflating: te-re-g/096.jpg         \n",
            "  inflating: te-re-g/097.jpg         \n",
            "  inflating: te-re-g/098.jpg         \n",
            "  inflating: te-re-g/099.jpg         \n",
            "  inflating: te-re-g/100.jpg         \n",
            "  inflating: te-re-g/101.jpg         \n",
            "  inflating: te-re-g/102.jpg         \n",
            "  inflating: te-re-g/103.jpg         \n",
            "  inflating: te-re-g/104.jpg         \n",
            "  inflating: te-re-g/105.jpg         \n",
            "  inflating: te-re-g/106.jpg         \n",
            "  inflating: te-re-g/107.jpg         \n",
            "  inflating: te-re-g/108.jpg         \n",
            "  inflating: te-re-g/109.jpg         \n",
            "  inflating: te-re-g/110.jpg         \n",
            "  inflating: te-re-g/111.jpg         \n",
            "  inflating: te-re-g/112.jpg         \n",
            "  inflating: te-re-g/113.jpg         \n",
            "  inflating: te-re-g/114.jpg         \n",
            "  inflating: te-re-g/115.jpg         \n",
            "  inflating: te-re-g/116.jpg         \n",
            "  inflating: te-re-g/117.jpg         \n",
            "  inflating: te-re-g/118.jpg         \n",
            "  inflating: te-re-g/119.jpg         \n",
            "  inflating: te-re-g/120.jpg         \n",
            "  inflating: te-re-g/121.jpg         \n",
            "  inflating: te-re-g/122.jpg         \n",
            "  inflating: te-re-g/123.jpg         \n",
            "  inflating: te-re-g/124.jpg         \n",
            "  inflating: te-re-g/125.jpg         \n",
            "  inflating: te-re-g/126.jpg         \n",
            "  inflating: te-re-g/127.jpg         \n",
            "  inflating: te-re-g/128.jpg         \n",
            "  inflating: te-re-g/129.jpg         \n",
            "  inflating: te-re-g/130.jpg         \n",
            "  inflating: te-re-g/131.jpg         \n",
            "  inflating: te-re-g/132.jpg         \n",
            "  inflating: te-re-g/133.jpg         \n",
            "  inflating: te-re-g/134.jpg         \n",
            "  inflating: te-re-g/135.jpg         \n",
            "  inflating: te-re-g/136.jpg         \n",
            "  inflating: te-re-g/137.jpg         \n",
            "  inflating: te-re-g/138.jpg         \n",
            "  inflating: te-re-g/139.jpg         \n",
            "  inflating: te-re-g/140.jpg         \n",
            "  inflating: te-re-g/141.jpg         \n",
            "  inflating: te-re-g/142.jpg         \n",
            "  inflating: te-re-g/143.jpg         \n",
            "  inflating: te-re-g/144.jpg         \n",
            "  inflating: te-re-g/145.jpg         \n",
            "  inflating: te-re-g/146.jpg         \n",
            "  inflating: te-re-g/147.jpg         \n",
            "  inflating: te-re-g/148.jpg         \n",
            "  inflating: te-re-g/149.jpg         \n",
            "  inflating: te-re-g/150.jpg         \n",
            "  inflating: te-re-g/151.jpg         \n",
            "  inflating: te-re-g/152.jpg         \n",
            "  inflating: te-re-g/153.jpg         \n",
            "  inflating: te-re-g/154.jpg         \n",
            "  inflating: te-re-g/155.jpg         \n",
            "  inflating: te-re-g/156.jpg         \n",
            "  inflating: te-re-g/157.jpg         \n",
            "  inflating: te-re-g/158.jpg         \n",
            "  inflating: te-re-g/159.jpg         \n",
            "  inflating: te-re-g/160.jpg         \n",
            "  inflating: te-re-g/161.jpg         \n",
            "  inflating: te-re-g/162.jpg         \n",
            "  inflating: te-re-g/163.jpg         \n",
            "  inflating: te-re-g/164.jpg         \n",
            "  inflating: te-re-g/165.jpg         \n",
            "  inflating: te-re-g/166.jpg         \n",
            "  inflating: te-re-g/167.jpg         \n",
            "  inflating: te-re-g/168.jpg         \n",
            "  inflating: te-re-g/169.jpg         \n",
            "  inflating: te-re-g/170.jpg         \n",
            "  inflating: te-re-g/171.jpg         \n",
            "  inflating: te-re-g/172.jpg         \n",
            "  inflating: te-re-g/173.jpg         \n",
            "  inflating: te-re-g/174.jpg         \n",
            "  inflating: te-re-g/175.jpg         \n",
            "  inflating: te-re-g/176.jpg         \n",
            "  inflating: te-re-g/177.jpg         \n",
            "  inflating: te-re-g/178.jpg         \n",
            "  inflating: te-re-g/179.jpg         \n",
            "  inflating: te-re-g/180.jpg         \n",
            "  inflating: te-re-g/181.jpg         \n",
            "  inflating: te-re-g/182.jpg         \n",
            "  inflating: te-re-g/183.jpg         \n",
            "  inflating: te-re-g/184.jpg         \n",
            "  inflating: te-re-g/185.jpg         \n",
            "  inflating: te-re-g/186.jpg         \n",
            "  inflating: te-re-g/187.jpg         \n",
            "  inflating: te-re-g/188.jpg         \n",
            "  inflating: te-re-g/189.jpg         \n",
            "  inflating: te-re-g/190.jpg         \n",
            "  inflating: te-re-g/191.jpg         \n",
            "  inflating: te-re-g/192.jpg         \n",
            "  inflating: te-re-g/193.jpg         \n",
            "  inflating: te-re-g/194.jpg         \n",
            "  inflating: te-re-g/195.jpg         \n",
            "  inflating: te-re-g/196.jpg         \n",
            "  inflating: te-re-g/197.jpg         \n",
            "  inflating: te-re-g/198.jpg         \n",
            "  inflating: te-re-g/199.jpg         \n",
            "  inflating: te-re-g/200.jpg         \n",
            "  inflating: te-re-g/201.jpg         \n",
            "  inflating: te-re-g/202.jpg         \n",
            "  inflating: te-re-g/203.jpg         \n",
            "  inflating: te-re-g/204.jpg         \n",
            "  inflating: te-re-g/205.jpg         \n",
            "  inflating: te-re-g/206.jpg         \n",
            "  inflating: te-re-g/207.jpg         \n",
            "  inflating: te-re-g/208.jpg         \n",
            "  inflating: te-re-g/209.jpg         \n",
            "  inflating: te-re-g/210.jpg         \n",
            "  inflating: te-re-g/211.jpg         \n",
            "  inflating: te-re-g/212.jpg         \n",
            "  inflating: te-re-g/213.jpg         \n",
            "  inflating: te-re-g/214.jpg         \n",
            "  inflating: te-re-g/215.jpg         \n",
            "  inflating: te-re-g/216.jpg         \n",
            "  inflating: te-re-g/217.jpg         \n",
            "  inflating: te-re-g/218.jpg         \n",
            "  inflating: te-re-g/219.jpg         \n",
            "  inflating: te-re-g/220.jpg         \n",
            "  inflating: te-re-g/221.jpg         \n",
            "  inflating: te-re-g/222.jpg         \n",
            "  inflating: te-re-g/223.jpg         \n",
            "  inflating: te-re-g/224.jpg         \n",
            "  inflating: te-re-g/225.jpg         \n",
            "  inflating: te-re-g/226.jpg         \n",
            "  inflating: te-re-g/227.jpg         \n",
            "  inflating: te-re-g/228.jpg         \n",
            "  inflating: te-re-g/229.jpg         \n",
            "  inflating: te-re-g/230.jpg         \n",
            "  inflating: te-re-g/231.jpg         \n",
            "  inflating: te-re-g/232.jpg         \n",
            "  inflating: te-re-g/233.jpg         \n",
            "  inflating: te-re-g/234.jpg         \n",
            "  inflating: te-re-g/235.jpg         \n",
            "  inflating: te-re-g/236.jpg         \n",
            "  inflating: te-re-g/237.jpg         \n",
            "  inflating: te-re-g/238.jpg         \n",
            "  inflating: te-re-g/239.jpg         \n",
            "  inflating: te-re-g/240.jpg         \n",
            "  inflating: te-re-g/241.jpg         \n",
            "  inflating: te-re-g/242.jpg         \n",
            "  inflating: te-re-g/243.jpg         \n",
            "  inflating: te-re-g/244.jpg         \n",
            "  inflating: te-re-g/245.jpg         \n",
            "  inflating: te-re-g/246.jpg         \n",
            "  inflating: te-re-g/247.jpg         \n",
            "  inflating: te-re-g/248.jpg         \n",
            "  inflating: te-re-g/249.jpg         \n",
            "  inflating: te-re-g/250.jpg         \n",
            "  inflating: te-re-g/251.jpg         \n",
            "  inflating: te-re-g/252.jpg         \n",
            "  inflating: te-re-g/253.jpg         \n",
            "  inflating: te-re-g/254.jpg         \n",
            "  inflating: te-re-g/255.jpg         \n",
            "  inflating: te-re-g/256.jpg         \n",
            "  inflating: te-re-g/257.jpg         \n",
            "  inflating: te-re-g/258.jpg         \n",
            "  inflating: te-re-g/259.jpg         \n",
            "  inflating: te-re-g/260.jpg         \n",
            "  inflating: te-re-g/261.jpg         \n",
            "  inflating: te-re-g/262.jpg         \n",
            "  inflating: te-re-g/263.jpg         \n",
            "  inflating: te-re-g/264.jpg         \n",
            "  inflating: te-re-g/265.jpg         \n",
            "  inflating: te-re-g/266.jpg         \n",
            "  inflating: te-re-g/267.jpg         \n",
            "  inflating: te-re-g/268.jpg         \n",
            "  inflating: te-re-g/269.jpg         \n",
            "  inflating: te-re-g/270.jpg         \n",
            "  inflating: te-re-g/271.jpg         \n",
            "  inflating: te-re-g/272.jpg         \n",
            "  inflating: te-re-g/273.jpg         \n",
            "  inflating: te-re-g/274.jpg         \n",
            "  inflating: te-re-g/275.jpg         \n",
            "  inflating: te-re-g/276.jpg         \n",
            "  inflating: te-re-g/277.jpg         \n",
            "  inflating: te-re-g/278.jpg         \n",
            "  inflating: te-re-g/279.jpg         \n",
            "  inflating: te-re-g/280.jpg         \n",
            "  inflating: te-re-g/281.jpg         \n",
            "  inflating: te-re-g/282.jpg         \n",
            "  inflating: te-re-g/283.jpg         \n",
            "  inflating: te-re-g/284.jpg         \n",
            "  inflating: te-re-g/285.jpg         \n",
            "  inflating: te-re-g/286.jpg         \n",
            "  inflating: te-re-g/287.jpg         \n",
            "  inflating: te-re-g/288.jpg         \n",
            "  inflating: te-re-g/289.jpg         \n",
            "  inflating: te-re-g/290.jpg         \n",
            "  inflating: te-re-g/291.jpg         \n",
            "  inflating: te-re-g/292.jpg         \n",
            "  inflating: te-re-g/293.jpg         \n",
            "  inflating: te-re-g/294.jpg         \n",
            "  inflating: te-re-g/295.jpg         \n",
            "  inflating: te-re-g/296.jpg         \n",
            "  inflating: te-re-g/297.jpg         \n",
            "  inflating: te-re-g/298.jpg         \n",
            "  inflating: te-re-g/299.jpg         \n",
            "  inflating: te-re-g/300.jpg         \n",
            "  inflating: te-re-g/301.jpg         \n",
            "  inflating: te-re-g/302.jpg         \n",
            "  inflating: te-re-g/303.jpg         \n",
            "  inflating: te-re-g/304.jpg         \n",
            "  inflating: te-re-g/305.jpg         \n",
            "  inflating: te-re-g/306.jpg         \n",
            "  inflating: te-re-g/307.jpg         \n",
            "  inflating: te-re-g/308.jpg         \n",
            "  inflating: te-re-g/309.jpg         \n",
            "  inflating: te-re-g/310.jpg         \n",
            "  inflating: te-re-g/311.jpg         \n",
            "  inflating: te-re-g/312.jpg         \n",
            "  inflating: te-re-g/313.jpg         \n",
            "  inflating: te-re-g/314.jpg         \n",
            "  inflating: te-re-g/315.jpg         \n",
            "  inflating: te-re-g/316.jpg         \n",
            "  inflating: te-re-g/317.jpg         \n",
            "  inflating: te-re-g/318.jpg         \n",
            "  inflating: te-re-g/319.jpg         \n",
            "  inflating: te-re-g/320.jpg         \n",
            "  inflating: te-re-g/321.jpg         \n",
            "  inflating: te-re-g/322.jpg         \n",
            "  inflating: te-re-g/323.jpg         \n",
            "  inflating: te-re-g/324.jpg         \n",
            "  inflating: te-re-g/325.jpg         \n",
            "  inflating: te-re-g/326.jpg         \n",
            "  inflating: te-re-g/327.jpg         \n",
            "  inflating: te-re-g/328.jpg         \n",
            "  inflating: te-re-g/329.jpg         \n",
            "  inflating: tr-re-g-label           \n",
            "   creating: tr-re-g/\n",
            "  inflating: tr-re-g/000.jpg         \n",
            "  inflating: tr-re-g/001.jpg         \n",
            "  inflating: tr-re-g/002.jpg         \n",
            "  inflating: tr-re-g/003.jpg         \n",
            "  inflating: tr-re-g/004.jpg         \n",
            "  inflating: tr-re-g/005.jpg         \n",
            "  inflating: tr-re-g/006.jpg         \n",
            "  inflating: tr-re-g/007.jpg         \n",
            "  inflating: tr-re-g/008.jpg         \n",
            "  inflating: tr-re-g/009.jpg         \n",
            "  inflating: tr-re-g/010.jpg         \n",
            "  inflating: tr-re-g/011.jpg         \n",
            "  inflating: tr-re-g/012.jpg         \n",
            "  inflating: tr-re-g/013.jpg         \n",
            "  inflating: tr-re-g/014.jpg         \n",
            "  inflating: tr-re-g/015.jpg         \n",
            "  inflating: tr-re-g/016.jpg         \n",
            "  inflating: tr-re-g/017.jpg         \n",
            "  inflating: tr-re-g/018.jpg         \n",
            "  inflating: tr-re-g/019.jpg         \n",
            "  inflating: tr-re-g/020.jpg         \n",
            "  inflating: tr-re-g/021.jpg         \n",
            "  inflating: tr-re-g/022.jpg         \n",
            "  inflating: tr-re-g/023.jpg         \n",
            "  inflating: tr-re-g/024.jpg         \n",
            "  inflating: tr-re-g/025.jpg         \n",
            "  inflating: tr-re-g/026.jpg         \n",
            "  inflating: tr-re-g/027.jpg         \n",
            "  inflating: tr-re-g/028.jpg         \n",
            "  inflating: tr-re-g/029.jpg         \n",
            "  inflating: tr-re-g/030.jpg         \n",
            "  inflating: tr-re-g/031.jpg         \n",
            "  inflating: tr-re-g/032.jpg         \n",
            "  inflating: tr-re-g/033.jpg         \n",
            "  inflating: tr-re-g/034.jpg         \n",
            "  inflating: tr-re-g/035.jpg         \n",
            "  inflating: tr-re-g/036.jpg         \n",
            "  inflating: tr-re-g/037.jpg         \n",
            "  inflating: tr-re-g/038.jpg         \n",
            "  inflating: tr-re-g/039.jpg         \n",
            "  inflating: tr-re-g/040.jpg         \n",
            "  inflating: tr-re-g/041.jpg         \n",
            "  inflating: tr-re-g/042.jpg         \n",
            "  inflating: tr-re-g/043.jpg         \n",
            "  inflating: tr-re-g/044.jpg         \n",
            "  inflating: tr-re-g/045.jpg         \n",
            "  inflating: tr-re-g/046.jpg         \n",
            "  inflating: tr-re-g/047.jpg         \n",
            "  inflating: tr-re-g/048.jpg         \n",
            "  inflating: tr-re-g/049.jpg         \n",
            "  inflating: tr-re-g/050.jpg         \n",
            "  inflating: tr-re-g/051.jpg         \n",
            "  inflating: tr-re-g/052.jpg         \n",
            "  inflating: tr-re-g/053.jpg         \n",
            "  inflating: tr-re-g/054.jpg         \n",
            "  inflating: tr-re-g/055.jpg         \n",
            "  inflating: tr-re-g/056.jpg         \n",
            "  inflating: tr-re-g/057.jpg         \n",
            "  inflating: tr-re-g/058.jpg         \n",
            "  inflating: tr-re-g/059.jpg         \n",
            "  inflating: tr-re-g/060.jpg         \n",
            "  inflating: tr-re-g/061.jpg         \n",
            "  inflating: tr-re-g/062.jpg         \n",
            "  inflating: tr-re-g/063.jpg         \n",
            "  inflating: tr-re-g/064.jpg         \n",
            "  inflating: tr-re-g/065.jpg         \n",
            "  inflating: tr-re-g/066.jpg         \n",
            "  inflating: tr-re-g/067.jpg         \n",
            "  inflating: tr-re-g/068.jpg         \n",
            "  inflating: tr-re-g/069.jpg         \n",
            "  inflating: tr-re-g/070.jpg         \n",
            "  inflating: tr-re-g/071.jpg         \n",
            "  inflating: tr-re-g/072.jpg         \n",
            "  inflating: tr-re-g/073.jpg         \n",
            "  inflating: tr-re-g/074.jpg         \n",
            "  inflating: tr-re-g/075.jpg         \n",
            "  inflating: tr-re-g/076.jpg         \n",
            "  inflating: tr-re-g/077.jpg         \n",
            "  inflating: tr-re-g/078.jpg         \n",
            "  inflating: tr-re-g/079.jpg         \n",
            "  inflating: tr-re-g/080.jpg         \n",
            "  inflating: tr-re-g/081.jpg         \n",
            "  inflating: tr-re-g/082.jpg         \n",
            "  inflating: tr-re-g/083.jpg         \n",
            "  inflating: tr-re-g/084.jpg         \n",
            "  inflating: tr-re-g/085.jpg         \n",
            "  inflating: tr-re-g/086.jpg         \n",
            "  inflating: tr-re-g/087.jpg         \n",
            "  inflating: tr-re-g/088.jpg         \n",
            "  inflating: tr-re-g/089.jpg         \n",
            "  inflating: tr-re-g/090.jpg         \n",
            "  inflating: tr-re-g/091.jpg         \n",
            "  inflating: tr-re-g/092.jpg         \n",
            "  inflating: tr-re-g/093.jpg         \n",
            "  inflating: tr-re-g/094.jpg         \n",
            "  inflating: tr-re-g/095.jpg         \n",
            "  inflating: tr-re-g/096.jpg         \n",
            "  inflating: tr-re-g/097.jpg         \n",
            "  inflating: tr-re-g/098.jpg         \n",
            "  inflating: tr-re-g/099.jpg         \n",
            "  inflating: tr-re-g/100.jpg         \n",
            "  inflating: tr-re-g/101.jpg         \n",
            "  inflating: tr-re-g/102.jpg         \n",
            "  inflating: tr-re-g/103.jpg         \n",
            "  inflating: tr-re-g/104.jpg         \n",
            "  inflating: tr-re-g/105.jpg         \n",
            "  inflating: tr-re-g/106.jpg         \n",
            "  inflating: tr-re-g/107.jpg         \n",
            "  inflating: tr-re-g/108.jpg         \n",
            "  inflating: tr-re-g/109.jpg         \n",
            "  inflating: tr-re-g/110.jpg         \n",
            "  inflating: tr-re-g/111.jpg         \n",
            "  inflating: tr-re-g/112.jpg         \n",
            "  inflating: tr-re-g/113.jpg         \n",
            "  inflating: tr-re-g/114.jpg         \n",
            "  inflating: tr-re-g/115.jpg         \n",
            "  inflating: tr-re-g/116.jpg         \n",
            "  inflating: tr-re-g/117.jpg         \n",
            "  inflating: tr-re-g/118.jpg         \n",
            "  inflating: tr-re-g/119.jpg         \n",
            "  inflating: tr-re-g/120.jpg         \n",
            "  inflating: tr-re-g/121.jpg         \n",
            "  inflating: tr-re-g/122.jpg         \n",
            "  inflating: tr-re-g/123.jpg         \n",
            "  inflating: tr-re-g/124.jpg         \n",
            "  inflating: tr-re-g/125.jpg         \n",
            "  inflating: tr-re-g/126.jpg         \n",
            "  inflating: tr-re-g/127.jpg         \n",
            "  inflating: tr-re-g/128.jpg         \n",
            "  inflating: tr-re-g/129.jpg         \n",
            "  inflating: tr-re-g/130.jpg         \n",
            "  inflating: tr-re-g/131.jpg         \n",
            "  inflating: tr-re-g/132.jpg         \n",
            "  inflating: tr-re-g/133.jpg         \n",
            "  inflating: tr-re-g/134.jpg         \n",
            "  inflating: tr-re-g/135.jpg         \n",
            "  inflating: tr-re-g/136.jpg         \n",
            "  inflating: tr-re-g/137.jpg         \n",
            "  inflating: tr-re-g/138.jpg         \n",
            "  inflating: tr-re-g/139.jpg         \n",
            "  inflating: tr-re-g/140.jpg         \n",
            "  inflating: tr-re-g/141.jpg         \n",
            "  inflating: tr-re-g/142.jpg         \n",
            "  inflating: tr-re-g/143.jpg         \n",
            "  inflating: tr-re-g/144.jpg         \n",
            "  inflating: tr-re-g/145.jpg         \n",
            "  inflating: tr-re-g/146.jpg         \n",
            "  inflating: tr-re-g/147.jpg         \n",
            "  inflating: tr-re-g/148.jpg         \n",
            "  inflating: tr-re-g/149.jpg         \n",
            "  inflating: tr-re-g/150.jpg         \n",
            "  inflating: tr-re-g/151.jpg         \n",
            "  inflating: tr-re-g/152.jpg         \n",
            "  inflating: tr-re-g/153.jpg         \n",
            "  inflating: tr-re-g/154.jpg         \n",
            "  inflating: tr-re-g/155.jpg         \n",
            "  inflating: tr-re-g/156.jpg         \n",
            "  inflating: tr-re-g/157.jpg         \n",
            "  inflating: tr-re-g/158.jpg         \n",
            "  inflating: tr-re-g/159.jpg         \n",
            "  inflating: tr-re-g/160.jpg         \n",
            "  inflating: tr-re-g/161.jpg         \n",
            "  inflating: tr-re-g/162.jpg         \n",
            "  inflating: tr-re-g/163.jpg         \n",
            "  inflating: tr-re-g/164.jpg         \n",
            "  inflating: tr-re-g/165.jpg         \n",
            "  inflating: tr-re-g/166.jpg         \n",
            "  inflating: tr-re-g/167.jpg         \n",
            "  inflating: tr-re-g/168.jpg         \n",
            "  inflating: tr-re-g/169.jpg         \n",
            "  inflating: tr-re-g/170.jpg         \n",
            "  inflating: tr-re-g/171.jpg         \n",
            "  inflating: tr-re-g/172.jpg         \n",
            "  inflating: tr-re-g/173.jpg         \n",
            "  inflating: tr-re-g/174.jpg         \n",
            "  inflating: tr-re-g/175.jpg         \n",
            "  inflating: tr-re-g/176.jpg         \n",
            "  inflating: tr-re-g/177.jpg         \n",
            "  inflating: tr-re-g/178.jpg         \n",
            "  inflating: tr-re-g/179.jpg         \n",
            "  inflating: tr-re-g/180.jpg         \n",
            "  inflating: tr-re-g/181.jpg         \n",
            "  inflating: tr-re-g/182.jpg         \n",
            "  inflating: tr-re-g/183.jpg         \n",
            "  inflating: tr-re-g/184.jpg         \n",
            "  inflating: tr-re-g/185.jpg         \n",
            "  inflating: tr-re-g/186.jpg         \n",
            "  inflating: tr-re-g/187.jpg         \n",
            "  inflating: tr-re-g/188.jpg         \n",
            "  inflating: tr-re-g/189.jpg         \n",
            "  inflating: tr-re-g/190.jpg         \n",
            "  inflating: tr-re-g/191.jpg         \n",
            "  inflating: tr-re-g/192.jpg         \n",
            "  inflating: tr-re-g/193.jpg         \n",
            "  inflating: tr-re-g/194.jpg         \n",
            "  inflating: tr-re-g/195.jpg         \n",
            "  inflating: tr-re-g/196.jpg         \n",
            "  inflating: tr-re-g/197.jpg         \n",
            "  inflating: tr-re-g/198.jpg         \n",
            "  inflating: tr-re-g/199.jpg         \n",
            "  inflating: tr-re-g/200.jpg         \n",
            "  inflating: tr-re-g/201.jpg         \n",
            "  inflating: tr-re-g/202.jpg         \n",
            "  inflating: tr-re-g/203.jpg         \n",
            "  inflating: tr-re-g/204.jpg         \n",
            "  inflating: tr-re-g/205.jpg         \n",
            "  inflating: tr-re-g/206.jpg         \n",
            "  inflating: tr-re-g/207.jpg         \n",
            "  inflating: tr-re-g/208.jpg         \n",
            "  inflating: tr-re-g/209.jpg         \n",
            "  inflating: tr-re-g/210.jpg         \n",
            "  inflating: tr-re-g/211.jpg         \n",
            "  inflating: tr-re-g/212.jpg         \n",
            "  inflating: tr-re-g/213.jpg         \n",
            "  inflating: tr-re-g/214.jpg         \n",
            "  inflating: tr-re-g/215.jpg         \n",
            "  inflating: tr-re-g/216.jpg         \n",
            "  inflating: tr-re-g/217.jpg         \n",
            "  inflating: tr-re-g/218.jpg         \n",
            "  inflating: tr-re-g/219.jpg         \n",
            "  inflating: tr-re-g/220.jpg         \n",
            "  inflating: tr-re-g/221.jpg         \n",
            "  inflating: tr-re-g/222.jpg         \n",
            "  inflating: tr-re-g/223.jpg         \n",
            "  inflating: tr-re-g/224.jpg         \n",
            "  inflating: tr-re-g/225.jpg         \n",
            "  inflating: tr-re-g/226.jpg         \n",
            "  inflating: tr-re-g/227.jpg         \n",
            "  inflating: tr-re-g/228.jpg         \n",
            "  inflating: tr-re-g/229.jpg         \n",
            "  inflating: tr-re-g/230.jpg         \n",
            "  inflating: tr-re-g/231.jpg         \n",
            "  inflating: tr-re-g/232.jpg         \n",
            "  inflating: tr-re-g/233.jpg         \n",
            "  inflating: tr-re-g/234.jpg         \n",
            "  inflating: tr-re-g/235.jpg         \n",
            "  inflating: tr-re-g/236.jpg         \n",
            "  inflating: tr-re-g/237.jpg         \n",
            "  inflating: tr-re-g/238.jpg         \n",
            "  inflating: tr-re-g/239.jpg         \n",
            "  inflating: tr-re-g/240.jpg         \n",
            "  inflating: tr-re-g/241.jpg         \n",
            "  inflating: tr-re-g/242.jpg         \n",
            "  inflating: tr-re-g/243.jpg         \n",
            "  inflating: tr-re-g/244.jpg         \n",
            "  inflating: tr-re-g/245.jpg         \n",
            "  inflating: tr-re-g/246.jpg         \n",
            "  inflating: tr-re-g/247.jpg         \n",
            "  inflating: tr-re-g/248.jpg         \n",
            "  inflating: tr-re-g/249.jpg         \n",
            "  inflating: tr-re-g/250.jpg         \n",
            "  inflating: tr-re-g/251.jpg         \n",
            "  inflating: tr-re-g/252.jpg         \n",
            "  inflating: tr-re-g/253.jpg         \n",
            "  inflating: tr-re-g/254.jpg         \n",
            "  inflating: tr-re-g/255.jpg         \n",
            "  inflating: tr-re-g/256.jpg         \n",
            "  inflating: tr-re-g/257.jpg         \n",
            "  inflating: tr-re-g/258.jpg         \n",
            "  inflating: tr-re-g/259.jpg         \n",
            "  inflating: tr-re-g/260.jpg         \n",
            "  inflating: tr-re-g/261.jpg         \n",
            "  inflating: tr-re-g/262.jpg         \n",
            "  inflating: tr-re-g/263.jpg         \n",
            "  inflating: tr-re-g/264.jpg         \n",
            "  inflating: tr-re-g/265.jpg         \n",
            "  inflating: tr-re-g/266.jpg         \n",
            "  inflating: tr-re-g/267.jpg         \n",
            "  inflating: tr-re-g/268.jpg         \n",
            "  inflating: tr-re-g/269.jpg         \n",
            "  inflating: tr-re-g/270.jpg         \n",
            "  inflating: tr-re-g/271.jpg         \n",
            "  inflating: tr-re-g/272.jpg         \n",
            "  inflating: tr-re-g/273.jpg         \n",
            "  inflating: tr-re-g/274.jpg         \n",
            "  inflating: tr-re-g/275.jpg         \n",
            "  inflating: tr-re-g/276.jpg         \n",
            "  inflating: tr-re-g/277.jpg         \n",
            "  inflating: tr-re-g/278.jpg         \n",
            "  inflating: tr-re-g/279.jpg         \n",
            "  inflating: tr-re-g/280.jpg         \n",
            "  inflating: tr-re-g/281.jpg         \n",
            "  inflating: tr-re-g/282.jpg         \n",
            "  inflating: tr-re-g/283.jpg         \n",
            "  inflating: tr-re-g/284.jpg         \n",
            "  inflating: tr-re-g/285.jpg         \n",
            "  inflating: tr-re-g/286.jpg         \n",
            "  inflating: tr-re-g/287.jpg         \n",
            "  inflating: tr-re-g/288.jpg         \n",
            "  inflating: tr-re-g/289.jpg         \n",
            "  inflating: tr-re-g/290.jpg         \n",
            "  inflating: tr-re-g/291.jpg         \n",
            "  inflating: tr-re-g/292.jpg         \n",
            "  inflating: tr-re-g/293.jpg         \n",
            "  inflating: tr-re-g/294.jpg         \n",
            "  inflating: tr-re-g/295.jpg         \n",
            "  inflating: tr-re-g/296.jpg         \n",
            "  inflating: tr-re-g/297.jpg         \n",
            "  inflating: tr-re-g/298.jpg         \n",
            "  inflating: tr-re-g/299.jpg         \n",
            "  inflating: tr-re-g/300.jpg         \n",
            "  inflating: tr-re-g/301.jpg         \n",
            "  inflating: tr-re-g/302.jpg         \n",
            "  inflating: tr-re-g/303.jpg         \n",
            "  inflating: tr-re-g/304.jpg         \n",
            "  inflating: tr-re-g/305.jpg         \n",
            "  inflating: tr-re-g/306.jpg         \n",
            "  inflating: tr-re-g/307.jpg         \n",
            "  inflating: tr-re-g/308.jpg         \n",
            "  inflating: tr-re-g/309.jpg         \n",
            "  inflating: tr-re-g/310.jpg         \n",
            "  inflating: tr-re-g/311.jpg         \n",
            "  inflating: tr-re-g/312.jpg         \n",
            "  inflating: tr-re-g/313.jpg         \n",
            "  inflating: tr-re-g/314.jpg         \n",
            "  inflating: tr-re-g/315.jpg         \n",
            "  inflating: tr-re-g/316.jpg         \n",
            "  inflating: tr-re-g/317.jpg         \n",
            "  inflating: tr-re-g/318.jpg         \n",
            "  inflating: tr-re-g/319.jpg         \n",
            "  inflating: tr-re-g/320.jpg         \n",
            "  inflating: tr-re-g/321.jpg         \n",
            "  inflating: tr-re-g/322.jpg         \n",
            "  inflating: tr-re-g/323.jpg         \n",
            "  inflating: tr-re-g/324.jpg         \n",
            "  inflating: tr-re-g/325.jpg         \n",
            "  inflating: tr-re-g/326.jpg         \n",
            "  inflating: tr-re-g/327.jpg         \n",
            "  inflating: tr-re-g/328.jpg         \n",
            "  inflating: tr-re-g/329.jpg         \n",
            "  inflating: tr-re-g/330.jpg         \n",
            "  inflating: tr-re-g/331.jpg         \n",
            "  inflating: tr-re-g/332.jpg         \n",
            "  inflating: tr-re-g/333.jpg         \n",
            "  inflating: tr-re-g/334.jpg         \n",
            "  inflating: tr-re-g/335.jpg         \n",
            "  inflating: tr-re-g/336.jpg         \n",
            "  inflating: tr-re-g/337.jpg         \n",
            "  inflating: tr-re-g/338.jpg         \n",
            "  inflating: tr-re-g/339.jpg         \n",
            "  inflating: tr-re-g/340.jpg         \n",
            "  inflating: tr-re-g/341.jpg         \n",
            "  inflating: tr-re-g/342.jpg         \n",
            "  inflating: tr-re-g/343.jpg         \n",
            "  inflating: tr-re-g/344.jpg         \n",
            "  inflating: tr-re-g/345.jpg         \n",
            "  inflating: tr-re-g/346.jpg         \n",
            "  inflating: tr-re-g/347.jpg         \n",
            "  inflating: tr-re-g/348.jpg         \n",
            "  inflating: tr-re-g/349.jpg         \n",
            "  inflating: tr-re-g/350.jpg         \n",
            "  inflating: tr-re-g/351.jpg         \n",
            "  inflating: tr-re-g/352.jpg         \n",
            "  inflating: tr-re-g/353.jpg         \n",
            "  inflating: tr-re-g/354.jpg         \n",
            "  inflating: tr-re-g/355.jpg         \n",
            "  inflating: tr-re-g/356.jpg         \n",
            "  inflating: tr-re-g/357.jpg         \n",
            "  inflating: tr-re-g/358.jpg         \n",
            "  inflating: tr-re-g/359.jpg         \n",
            "  inflating: tr-re-g/360.jpg         \n",
            "  inflating: tr-re-g/361.jpg         \n",
            "  inflating: tr-re-g/362.jpg         \n",
            "  inflating: tr-re-g/363.jpg         \n",
            "  inflating: tr-re-g/364.jpg         \n",
            "  inflating: tr-re-g/365.jpg         \n",
            "  inflating: tr-re-g/366.jpg         \n",
            "  inflating: tr-re-g/367.jpg         \n",
            "  inflating: tr-re-g/368.jpg         \n",
            "  inflating: tr-re-g/369.jpg         \n",
            "  inflating: tr-re-g/370.jpg         \n",
            "  inflating: tr-re-g/371.jpg         \n",
            "  inflating: tr-re-g/372.jpg         \n",
            "  inflating: tr-re-g/373.jpg         \n",
            "  inflating: tr-re-g/374.jpg         \n",
            "  inflating: tr-re-g/375.jpg         \n",
            "  inflating: tr-re-g/376.jpg         \n",
            "  inflating: tr-re-g/377.jpg         \n",
            "  inflating: tr-re-g/378.jpg         \n",
            "  inflating: tr-re-g/379.jpg         \n",
            "  inflating: tr-re-g/380.jpg         \n",
            "  inflating: tr-re-g/381.jpg         \n",
            "  inflating: tr-re-g/382.jpg         \n",
            "  inflating: tr-re-g/383.jpg         \n",
            "  inflating: tr-re-g/384.jpg         \n",
            "  inflating: tr-re-g/385.jpg         \n",
            "  inflating: tr-re-g/386.jpg         \n",
            "  inflating: tr-re-g/387.jpg         \n",
            "  inflating: tr-re-g/388.jpg         \n",
            "  inflating: tr-re-g/389.jpg         \n",
            "  inflating: tr-re-g/390.jpg         \n",
            "  inflating: tr-re-g/391.jpg         \n",
            "  inflating: tr-re-g/392.jpg         \n",
            "  inflating: tr-re-g/393.jpg         \n",
            "  inflating: tr-re-g/394.jpg         \n",
            "  inflating: tr-re-g/395.jpg         \n",
            "  inflating: tr-re-g/396.jpg         \n",
            "  inflating: tr-re-g/397.jpg         \n",
            "  inflating: tr-re-g/398.jpg         \n",
            "  inflating: tr-re-g/399.jpg         \n",
            "  inflating: tr-re-g/400.jpg         \n",
            "  inflating: tr-re-g/401.jpg         \n",
            "  inflating: tr-re-g/402.jpg         \n",
            "  inflating: tr-re-g/403.jpg         \n",
            "  inflating: tr-re-g/404.jpg         \n",
            "  inflating: tr-re-g/405.jpg         \n",
            "  inflating: tr-re-g/406.jpg         \n",
            "  inflating: tr-re-g/407.jpg         \n",
            "  inflating: tr-re-g/408.jpg         \n",
            "  inflating: tr-re-g/409.jpg         \n",
            "  inflating: tr-re-g/410.jpg         \n",
            "  inflating: tr-re-g/411.jpg         \n",
            "  inflating: tr-re-g/412.jpg         \n",
            "  inflating: tr-re-g/413.jpg         \n",
            "  inflating: tr-re-g/414.jpg         \n",
            "  inflating: tr-re-g/415.jpg         \n",
            "  inflating: tr-re-g/416.jpg         \n",
            "  inflating: tr-re-g/417.jpg         \n",
            "  inflating: tr-re-g/418.jpg         \n",
            "  inflating: tr-re-g/419.jpg         \n",
            "  inflating: tr-re-g/420.jpg         \n",
            "  inflating: tr-re-g/421.jpg         \n",
            "  inflating: tr-re-g/422.jpg         \n",
            "  inflating: tr-re-g/423.jpg         \n",
            "  inflating: tr-re-g/424.jpg         \n",
            "  inflating: tr-re-g/425.jpg         \n",
            "  inflating: tr-re-g/426.jpg         \n",
            "  inflating: tr-re-g/427.jpg         \n",
            "  inflating: tr-re-g/428.jpg         \n",
            "  inflating: tr-re-g/429.jpg         \n",
            "  inflating: tr-re-g/430.jpg         \n",
            "  inflating: tr-re-g/431.jpg         \n",
            "  inflating: tr-re-g/432.jpg         \n",
            "  inflating: tr-re-g/433.jpg         \n",
            "  inflating: tr-re-g/434.jpg         \n",
            "  inflating: tr-re-g/435.jpg         \n",
            "  inflating: tr-re-g/436.jpg         \n",
            "  inflating: tr-re-g/437.jpg         \n",
            "  inflating: tr-re-g/438.jpg         \n",
            "  inflating: tr-re-g/439.jpg         \n",
            "  inflating: tr-re-g/440.jpg         \n",
            "  inflating: tr-re-g/441.jpg         \n",
            "  inflating: tr-re-g/442.jpg         \n",
            "  inflating: tr-re-g/443.jpg         \n",
            "  inflating: tr-re-g/444.jpg         \n",
            "  inflating: tr-re-g/445.jpg         \n",
            "  inflating: tr-re-g/446.jpg         \n",
            "  inflating: tr-re-g/447.jpg         \n",
            "  inflating: tr-re-g/448.jpg         \n",
            "  inflating: tr-re-g/449.jpg         \n",
            "  inflating: tr-re-g/450.jpg         \n",
            "  inflating: tr-re-g/451.jpg         \n",
            "  inflating: tr-re-g/452.jpg         \n",
            "  inflating: tr-re-g/453.jpg         \n",
            "  inflating: tr-re-g/454.jpg         \n",
            "  inflating: tr-re-g/455.jpg         \n",
            "  inflating: tr-re-g/456.jpg         \n",
            "  inflating: tr-re-g/457.jpg         \n",
            "  inflating: tr-re-g/458.jpg         \n",
            "  inflating: tr-re-g/459.jpg         \n",
            "  inflating: tr-re-g/460.jpg         \n",
            "  inflating: tr-re-g/461.jpg         \n",
            "  inflating: tr-re-g/462.jpg         \n",
            "  inflating: tr-re-g/463.jpg         \n",
            "  inflating: tr-re-g/464.jpg         \n",
            "  inflating: tr-re-g/465.jpg         \n",
            "  inflating: tr-re-g/466.jpg         \n",
            "  inflating: tr-re-g/467.jpg         \n",
            "  inflating: tr-re-g/468.jpg         \n",
            "  inflating: tr-re-g/469.jpg         \n",
            "  inflating: tr-re-g/470.jpg         \n",
            "  inflating: tr-re-g/471.jpg         \n",
            "  inflating: tr-re-g/472.jpg         \n",
            "  inflating: tr-re-g/473.jpg         \n",
            "  inflating: tr-re-g/474.jpg         \n",
            "  inflating: tr-re-g/475.jpg         \n",
            "  inflating: tr-re-g/476.jpg         \n",
            "  inflating: tr-re-g/477.jpg         \n",
            "  inflating: tr-re-g/478.jpg         \n",
            "  inflating: tr-re-g/479.jpg         \n",
            "  inflating: tr-re-g/480.jpg         \n",
            "  inflating: tr-re-g/481.jpg         \n",
            "  inflating: tr-re-g/482.jpg         \n",
            "  inflating: tr-re-g/483.jpg         \n",
            "  inflating: tr-re-g/484.jpg         \n",
            "  inflating: tr-re-g/485.jpg         \n",
            "  inflating: tr-re-g/486.jpg         \n",
            "  inflating: tr-re-g/487.jpg         \n",
            "  inflating: tr-re-g/488.jpg         \n",
            "  inflating: tr-re-g/489.jpg         \n",
            "  inflating: tr-re-g/490.jpg         \n",
            "  inflating: tr-re-g/491.jpg         \n",
            "  inflating: tr-re-g/492.jpg         \n",
            "  inflating: tr-re-g/493.jpg         \n",
            "  inflating: tr-re-g/494.jpg         \n",
            "  inflating: tr-re-g/495.jpg         \n",
            "  inflating: tr-re-g/496.jpg         \n",
            "  inflating: tr-re-g/497.jpg         \n",
            "  inflating: tr-re-g/498.jpg         \n",
            "  inflating: tr-re-g/499.jpg         \n",
            "  inflating: tr-re-g/500.jpg         \n",
            "  inflating: tr-re-g/501.jpg         \n",
            "  inflating: tr-re-g/502.jpg         \n",
            "  inflating: tr-re-g/503.jpg         \n",
            "  inflating: tr-re-g/504.jpg         \n",
            "  inflating: tr-re-g/505.jpg         \n",
            "  inflating: tr-re-g/506.jpg         \n",
            "  inflating: tr-re-g/507.jpg         \n",
            "  inflating: tr-re-g/508.jpg         \n",
            "  inflating: tr-re-g/509.jpg         \n",
            "  inflating: tr-re-g/510.jpg         \n",
            "  inflating: tr-re-g/511.jpg         \n",
            "  inflating: tr-re-g/512.jpg         \n",
            "  inflating: tr-re-g/513.jpg         \n",
            "  inflating: tr-re-g/514.jpg         \n",
            "  inflating: tr-re-g/515.jpg         \n",
            "  inflating: tr-re-g/516.jpg         \n",
            "  inflating: tr-re-g/517.jpg         \n",
            "  inflating: tr-re-g/518.jpg         \n",
            "  inflating: tr-re-g/519.jpg         \n",
            "  inflating: tr-re-g/520.jpg         \n",
            "  inflating: tr-re-g/521.jpg         \n",
            "  inflating: tr-re-g/522.jpg         \n",
            "  inflating: tr-re-g/523.jpg         \n",
            "  inflating: tr-re-g/524.jpg         \n",
            "  inflating: tr-re-g/525.jpg         \n",
            "  inflating: tr-re-g/526.jpg         \n",
            "  inflating: tr-re-g/527.jpg         \n",
            "  inflating: tr-re-g/528.jpg         \n",
            "  inflating: tr-re-g/529.jpg         \n",
            "  inflating: tr-re-g/530.jpg         \n",
            "  inflating: tr-re-g/531.jpg         \n",
            "  inflating: tr-re-g/532.jpg         \n",
            "  inflating: tr-re-g/533.jpg         \n",
            "  inflating: tr-re-g/534.jpg         \n",
            "  inflating: tr-re-g/535.jpg         \n",
            "  inflating: tr-re-g/536.jpg         \n",
            "  inflating: tr-re-g/537.jpg         \n",
            "  inflating: tr-re-g/538.jpg         \n",
            "  inflating: tr-re-g/539.jpg         \n",
            "  inflating: tr-re-g/540.jpg         \n",
            "  inflating: tr-re-g/541.jpg         \n",
            "  inflating: tr-re-g/542.jpg         \n",
            "  inflating: tr-re-g/543.jpg         \n",
            "  inflating: tr-re-g/544.jpg         \n",
            "  inflating: tr-re-g/545.jpg         \n",
            "  inflating: tr-re-g/546.jpg         \n",
            "  inflating: tr-re-g/547.jpg         \n",
            "  inflating: tr-re-g/548.jpg         \n",
            "  inflating: tr-re-g/549.jpg         \n",
            "  inflating: tr-re-g/550.jpg         \n",
            "  inflating: tr-re-g/551.jpg         \n",
            "  inflating: tr-re-g/552.jpg         \n",
            "  inflating: tr-re-g/553.jpg         \n",
            "  inflating: tr-re-g/554.jpg         \n",
            "  inflating: tr-re-g/555.jpg         \n",
            "  inflating: tr-re-g/556.jpg         \n",
            "  inflating: tr-re-g/557.jpg         \n",
            "  inflating: tr-re-g/558.jpg         \n",
            "  inflating: tr-re-g/559.jpg         \n",
            "  inflating: tr-re-g/560.jpg         \n",
            "  inflating: tr-re-g/561.jpg         \n",
            "  inflating: tr-re-g/562.jpg         \n",
            "  inflating: tr-re-g/563.jpg         \n",
            "  inflating: tr-re-g/564.jpg         \n",
            "  inflating: tr-re-g/565.jpg         \n",
            "  inflating: tr-re-g/566.jpg         \n",
            "  inflating: tr-re-g/567.jpg         \n",
            "  inflating: tr-re-g/568.jpg         \n",
            "  inflating: tr-re-g/569.jpg         \n",
            "  inflating: tr-re-g/570.jpg         \n",
            "  inflating: tr-re-g/571.jpg         \n",
            "  inflating: tr-re-g/572.jpg         \n",
            "  inflating: tr-re-g/573.jpg         \n",
            "  inflating: tr-re-g/574.jpg         \n",
            "  inflating: tr-re-g/575.jpg         \n",
            "  inflating: tr-re-g/576.jpg         \n",
            "  inflating: tr-re-g/577.jpg         \n",
            "  inflating: tr-re-g/578.jpg         \n",
            "  inflating: tr-re-g/579.jpg         \n",
            "  inflating: tr-re-g/580.jpg         \n",
            "  inflating: tr-re-g/581.jpg         \n",
            "  inflating: tr-re-g/582.jpg         \n",
            "  inflating: tr-re-g/583.jpg         \n",
            "  inflating: tr-re-g/584.jpg         \n",
            "  inflating: tr-re-g/585.jpg         \n",
            "  inflating: tr-re-g/586.jpg         \n",
            "  inflating: tr-re-g/587.jpg         \n",
            "  inflating: tr-re-g/588.jpg         \n",
            "  inflating: tr-re-g/589.jpg         \n",
            "  inflating: tr-re-g/590.jpg         \n",
            "  inflating: tr-re-g/591.jpg         \n",
            "  inflating: tr-re-g/592.jpg         \n",
            "  inflating: tr-re-g/593.jpg         \n",
            "  inflating: tr-re-g/594.jpg         \n",
            "  inflating: tr-re-g/595.jpg         \n",
            "  inflating: tr-re-g/596.jpg         \n",
            "  inflating: tr-re-g/597.jpg         \n",
            "  inflating: tr-re-g/598.jpg         \n",
            "  inflating: tr-re-g/599.jpg         \n",
            "  inflating: tr-re-g/600.jpg         \n",
            "  inflating: tr-re-g/601.jpg         \n",
            "  inflating: tr-re-g/602.jpg         \n",
            "  inflating: tr-re-g/603.jpg         \n",
            "  inflating: tr-re-g/604.jpg         \n",
            "  inflating: tr-re-g/605.jpg         \n",
            "  inflating: tr-re-g/606.jpg         \n",
            "  inflating: tr-re-g/607.jpg         \n",
            "  inflating: tr-re-g/608.jpg         \n",
            "  inflating: tr-re-g/609.jpg         \n",
            "  inflating: tr-re-g/610.jpg         \n",
            "  inflating: tr-re-g/611.jpg         \n",
            "  inflating: tr-re-g/612.jpg         \n",
            "  inflating: tr-re-g/613.jpg         \n",
            "  inflating: tr-re-g/614.jpg         \n",
            "  inflating: tr-re-g/615.jpg         \n",
            "  inflating: tr-re-g/616.jpg         \n",
            "  inflating: tr-re-g/617.jpg         \n",
            "  inflating: tr-re-g/618.jpg         \n",
            "  inflating: tr-re-g/619.jpg         \n",
            "  inflating: tr-re-g/620.jpg         \n",
            "  inflating: tr-re-g/621.jpg         \n",
            "  inflating: tr-re-g/622.jpg         \n",
            "  inflating: tr-re-g/623.jpg         \n",
            "  inflating: tr-re-g/624.jpg         \n",
            "  inflating: tr-re-g/625.jpg         \n",
            "  inflating: tr-re-g/626.jpg         \n",
            "  inflating: tr-re-g/627.jpg         \n",
            "  inflating: tr-re-g/628.jpg         \n",
            "  inflating: tr-re-g/629.jpg         \n",
            "  inflating: tr-re-g/630.jpg         \n",
            "  inflating: tr-re-g/631.jpg         \n",
            "  inflating: tr-re-g/632.jpg         \n",
            "  inflating: tr-re-g/633.jpg         \n",
            "  inflating: tr-re-g/634.jpg         \n",
            "  inflating: tr-re-g/635.jpg         \n",
            "  inflating: tr-re-g/636.jpg         \n",
            "  inflating: tr-re-g/637.jpg         \n",
            "  inflating: tr-re-g/638.jpg         \n",
            "  inflating: tr-re-g/639.jpg         \n",
            "  inflating: tr-re-g/640.jpg         \n",
            "  inflating: tr-re-g/641.jpg         \n",
            "  inflating: tr-re-g/642.jpg         \n",
            "  inflating: tr-re-g/643.jpg         \n",
            "  inflating: tr-re-g/644.jpg         \n",
            "  inflating: tr-re-g/645.jpg         \n",
            "  inflating: tr-re-g/646.jpg         \n",
            "  inflating: tr-re-g/647.jpg         \n",
            "  inflating: tr-re-g/648.jpg         \n",
            "  inflating: tr-re-g/649.jpg         \n",
            "  inflating: tr-re-g/650.jpg         \n",
            "  inflating: tr-re-g/651.jpg         \n",
            "  inflating: tr-re-g/652.jpg         \n",
            "  inflating: tr-re-g/653.jpg         \n",
            "  inflating: tr-re-g/654.jpg         \n",
            "  inflating: tr-re-g/655.jpg         \n",
            "  inflating: tr-re-g/656.jpg         \n",
            "  inflating: tr-re-g/657.jpg         \n",
            "  inflating: tr-re-g/658.jpg         \n",
            "  inflating: tr-re-g/659.jpg         \n",
            "  inflating: tr-re-g/660.jpg         \n",
            "  inflating: tr-re-g/661.jpg         \n",
            "  inflating: tr-re-g/662.jpg         \n",
            "  inflating: tr-re-g/663.jpg         \n",
            "  inflating: tr-re-g/664.jpg         \n",
            "  inflating: tr-re-g/665.jpg         \n",
            "  inflating: tr-re-g/666.jpg         \n",
            "  inflating: tr-re-g/667.jpg         \n",
            "  inflating: tr-re-g/668.jpg         \n",
            "  inflating: tr-re-g/669.jpg         \n",
            "  inflating: tr-re-g/670.jpg         \n",
            "  inflating: tr-re-g/671.jpg         \n",
            "  inflating: tr-re-g/672.jpg         \n",
            "  inflating: tr-re-g/673.jpg         \n",
            "  inflating: tr-re-g/674.jpg         \n",
            "  inflating: tr-re-g/675.jpg         \n",
            "  inflating: tr-re-g/676.jpg         \n",
            "  inflating: tr-re-g/677.jpg         \n",
            "  inflating: tr-re-g/678.jpg         \n",
            "  inflating: tr-re-g/679.jpg         \n",
            "  inflating: tr-re-g/680.jpg         \n",
            "  inflating: tr-re-g/681.jpg         \n",
            "  inflating: tr-re-g/682.jpg         \n",
            "  inflating: tr-re-g/683.jpg         \n",
            "  inflating: tr-re-g/684.jpg         \n",
            "  inflating: tr-re-g/685.jpg         \n",
            "  inflating: tr-re-g/686.jpg         \n",
            "  inflating: tr-re-g/687.jpg         \n",
            "  inflating: tr-re-g/688.jpg         \n",
            "  inflating: tr-re-g/689.jpg         \n",
            "  inflating: tr-re-g/690.jpg         \n",
            "  inflating: tr-re-g/691.jpg         \n",
            "  inflating: tr-re-g/692.jpg         \n",
            "  inflating: tr-re-g/693.jpg         \n",
            "  inflating: tr-re-g/694.jpg         \n",
            "  inflating: tr-re-g/695.jpg         \n",
            "  inflating: tr-re-g/696.jpg         \n",
            "  inflating: tr-re-g/697.jpg         \n",
            "  inflating: tr-re-g/698.jpg         \n",
            "  inflating: tr-re-g/699.jpg         \n",
            "  inflating: tr-re-g/700.jpg         \n",
            "  inflating: tr-re-g/701.jpg         \n",
            "  inflating: tr-re-g/702.jpg         \n",
            "  inflating: tr-re-g/703.jpg         \n",
            "  inflating: tr-re-g/704.jpg         \n",
            "  inflating: tr-re-g/705.jpg         \n",
            "  inflating: tr-re-g/706.jpg         \n",
            "  inflating: tr-re-g/707.jpg         \n",
            "  inflating: tr-re-g/708.jpg         \n",
            "  inflating: tr-re-g/709.jpg         \n",
            "  inflating: tr-re-g/710.jpg         \n",
            "  inflating: tr-re-g/711.jpg         \n",
            "  inflating: tr-re-g/712.jpg         \n",
            "  inflating: tr-re-g/713.jpg         \n",
            "  inflating: tr-re-g/714.jpg         \n",
            "  inflating: tr-re-g/715.jpg         \n",
            "  inflating: tr-re-g/716.jpg         \n",
            "  inflating: tr-re-g/717.jpg         \n",
            "  inflating: tr-re-g/718.jpg         \n",
            "  inflating: tr-re-g/719.jpg         \n",
            "  inflating: tr-re-g/720.jpg         \n",
            "  inflating: tr-re-g/721.jpg         \n",
            "  inflating: tr-re-g/722.jpg         \n",
            "  inflating: tr-re-g/723.jpg         \n",
            "  inflating: tr-re-g/724.jpg         \n",
            "  inflating: tr-re-g/725.jpg         \n",
            "  inflating: tr-re-g/726.jpg         \n",
            "  inflating: tr-re-g/727.jpg         \n",
            "  inflating: tr-re-g/728.jpg         \n",
            "  inflating: tr-re-g/729.jpg         \n",
            "  inflating: tr-re-g/730.jpg         \n",
            "  inflating: tr-re-g/731.jpg         \n",
            "  inflating: tr-re-g/732.jpg         \n",
            "  inflating: tr-re-g/733.jpg         \n",
            "  inflating: tr-re-g/734.jpg         \n",
            "  inflating: tr-re-g/735.jpg         \n",
            "  inflating: tr-re-g/736.jpg         \n",
            "  inflating: tr-re-g/737.jpg         \n",
            "  inflating: tr-re-g/738.jpg         \n",
            "  inflating: tr-re-g/739.jpg         \n",
            "  inflating: tr-re-g/740.jpg         \n",
            "  inflating: tr-re-g/741.jpg         \n",
            "  inflating: tr-re-g/742.jpg         \n",
            "  inflating: tr-re-g/743.jpg         \n",
            "  inflating: tr-re-g/744.jpg         \n",
            "  inflating: tr-re-g/745.jpg         \n",
            "  inflating: tr-re-g/746.jpg         \n",
            "  inflating: tr-re-g/747.jpg         \n",
            "  inflating: tr-re-g/748.jpg         \n",
            "  inflating: tr-re-g/749.jpg         \n",
            "  inflating: tr-re-g/750.jpg         \n",
            "  inflating: tr-re-g/751.jpg         \n",
            "  inflating: tr-re-g/752.jpg         \n",
            "  inflating: tr-re-g/753.jpg         \n",
            "  inflating: tr-re-g/754.jpg         \n",
            "  inflating: tr-re-g/755.jpg         \n",
            "  inflating: tr-re-g/756.jpg         \n",
            "  inflating: tr-re-g/757.jpg         \n",
            "  inflating: tr-re-g/758.jpg         \n",
            "  inflating: tr-re-g/759.jpg         \n",
            "  inflating: tr-re-g/760.jpg         \n",
            "  inflating: tr-re-g/761.jpg         \n",
            "  inflating: tr-re-g/762.jpg         \n",
            "  inflating: tr-re-g/763.jpg         \n",
            "  inflating: tr-re-g/764.jpg         \n",
            "  inflating: tr-re-g/765.jpg         \n",
            "  inflating: tr-re-g/766.jpg         \n",
            "  inflating: tr-re-g/767.jpg         \n",
            "  inflating: tr-re-g/768.jpg         \n",
            "  inflating: tr-re-g/769.jpg         \n",
            "  inflating: tr-re-g/770.jpg         \n",
            "  inflating: tr-re-g/771.jpg         \n",
            "  inflating: tr-re-g/772.jpg         \n",
            "  inflating: tr-re-g/773.jpg         \n",
            "  inflating: tr-re-g/774.jpg         \n",
            "  inflating: tr-re-g/775.jpg         \n",
            "  inflating: tr-re-g/776.jpg         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eA-WAhhAnsLw",
        "outputId": "2af62043-381b-4ba0-ad8c-78962491fb7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 로딩 중...\n",
            "데이터 전처리 중...\n",
            "검증 데이터 생성 중...\n",
            "데이터 증강 중...\n",
            "원본 훈련 데이터: (700, 1, 64, 64)\n",
            "증강된 훈련 데이터: (1404, 1, 64, 64)\n",
            "검증 데이터: (77, 1, 64, 64)\n",
            "모델 훈련 시작...\n",
            "train loss:0.8153939675978611\n",
            "train loss:1.4371367614077708\n",
            "train loss:1.7769765701224545\n",
            "train loss:1.8315422056831834\n",
            "train loss:1.7913644282736292\n",
            "train loss:1.8551901400603974\n",
            "train loss:2.261958932949287\n",
            "train loss:1.7857466694744102\n",
            "train loss:2.0526478466052955\n",
            "train loss:2.445962651289731\n",
            "train loss:1.7976860233393932\n",
            "train loss:1.8091960516076486\n",
            "train loss:2.083951196037999\n",
            "train loss:1.920099462048473\n",
            "train loss:1.625847404547758\n",
            "train loss:1.7671537760042586\n",
            "train loss:1.63798275299111\n",
            "train loss:1.8265611759633127\n",
            "train loss:1.4961264629541629\n",
            "train loss:1.5970239108340092\n",
            "train loss:1.4807629473242268\n",
            "train loss:1.5377287753527569\n",
            "train loss:1.7584729125162273\n",
            "train loss:1.7931484522246839\n",
            "train loss:1.5547499094334714\n",
            "train loss:1.5312524820142306\n",
            "train loss:1.4733005439736697\n",
            "train loss:1.2267537443110794\n",
            "train loss:1.3045283048919925\n",
            "train loss:1.4368461593471373\n",
            "train loss:1.705858795309826\n",
            "train loss:1.1977745485678035\n",
            "train loss:1.2866203965748064\n",
            "train loss:1.3389887872223032\n",
            "train loss:0.9139689274613674\n",
            "train loss:1.0334947151703768\n",
            "train loss:1.2354356949411514\n",
            "train loss:1.33256556835074\n",
            "train loss:1.20561624130931\n",
            "train loss:1.4737423744593392\n",
            "train loss:1.218504586215721\n",
            "train loss:0.9838727427314365\n",
            "train loss:0.915653133134364\n",
            "train loss:1.037055257944281\n",
            "train loss:1.0634607823880238\n",
            "train loss:1.1089493472059528\n",
            "train loss:1.129585818934402\n",
            "train loss:0.9607093690447641\n",
            "train loss:0.994390373726975\n",
            "train loss:0.9689032210841076\n",
            "train loss:1.0258034525831943\n",
            "train loss:1.0062470157284318\n",
            "train loss:1.0523586583864188\n",
            "train loss:0.9743127106684071\n",
            "train loss:0.9568445473994867\n",
            "train loss:0.7224079245462283\n",
            "train loss:0.900678769025991\n",
            "train loss:0.9831087310896124\n",
            "train loss:0.9130036984165948\n",
            "train loss:1.1072812053666299\n",
            "train loss:0.7680480584334539\n",
            "train loss:0.7886042461032863\n",
            "train loss:0.7982573633013861\n",
            "train loss:0.732982402126354\n",
            "train loss:0.9862896910570548\n",
            "train loss:0.9368269836052815\n",
            "train loss:0.897384570623428\n",
            "train loss:1.001989185369873\n",
            "train loss:0.4443373877774961\n",
            "train loss:0.5344494923903491\n",
            "train loss:1.3862275806825624\n",
            "train loss:0.6525727475336219\n",
            "train loss:0.9531436144042524\n",
            "train loss:0.7817050199290596\n",
            "train loss:0.5968250658693721\n",
            "train loss:0.8380964394282956\n",
            "train loss:0.7520904976154139\n",
            "train loss:0.5877979589454089\n",
            "train loss:0.647705317462254\n",
            "train loss:0.5493844676043036\n",
            "train loss:0.7568174459059246\n",
            "train loss:0.49042508583819566\n",
            "train loss:0.8006856448451749\n",
            "train loss:1.0498201627894774\n",
            "train loss:0.5326080194849157\n",
            "train loss:0.4246020660787812\n",
            "train loss:0.4612363420162583\n",
            "train loss:0.7519375345156138\n",
            "train loss:0.5950086514716805\n",
            "train loss:0.5216360339835777\n",
            "train loss:0.644982155628598\n",
            "train loss:0.7172741803555882\n",
            "train loss:0.7608077021533806\n",
            "train loss:0.5141570662177705\n",
            "train loss:0.6067303750749324\n",
            "train loss:0.3245455099737229\n",
            "train loss:0.4612460620686264\n",
            "train loss:0.5139118437202342\n",
            "train loss:0.5743606402254898\n",
            "train loss:0.4371762354863564\n",
            "train loss:0.43303333725007065\n",
            "train loss:0.4314872586503172\n",
            "train loss:0.4631459784841665\n",
            "train loss:0.5414199538109679\n",
            "train loss:0.40361540825570197\n",
            "train loss:0.3940426519864949\n",
            "train loss:0.433712620190717\n",
            "train loss:0.4754481363429988\n",
            "train loss:0.24084580011017703\n",
            "train loss:0.3717870355096944\n",
            "train loss:0.49566857491215943\n",
            "train loss:0.5650385155281281\n",
            "train loss:0.3476149651861893\n",
            "train loss:0.48214000981629873\n",
            "train loss:0.2886083058791709\n",
            "train loss:0.572043765477664\n",
            "train loss:0.3413146975481831\n",
            "train loss:0.2069065022634441\n",
            "train loss:0.3786861136410373\n",
            "train loss:0.33078947532965786\n",
            "train loss:0.33222128695589387\n",
            "train loss:0.22278614524828283\n",
            "train loss:0.4570248430614521\n",
            "train loss:0.2297760646076567\n",
            "train loss:0.492896522409195\n",
            "train loss:0.25105657790403463\n",
            "train loss:0.3110435136342814\n",
            "train loss:0.3544083024964796\n",
            "train loss:0.19412336222716245\n",
            "train loss:0.23324014459938697\n",
            "train loss:0.4028625203080767\n",
            "train loss:0.24390928703298775\n",
            "train loss:0.24034457919449548\n",
            "train loss:0.3980423674638337\n",
            "train loss:0.34158781345359934\n",
            "train loss:0.3022038209247677\n",
            "train loss:0.3914732230493291\n",
            "train loss:0.2997712983724732\n",
            "train loss:0.18272166446807503\n",
            "train loss:0.2012695180612751\n",
            "train loss:0.28044653927197905\n",
            "train loss:0.4221820018973405\n",
            "train loss:0.5167843289214047\n",
            "train loss:0.3671857670452328\n",
            "train loss:0.26734141420653174\n",
            "train loss:0.3229790542385439\n",
            "train loss:0.34038151527425053\n",
            "train loss:0.26523980847351714\n",
            "train loss:0.2477571894332034\n",
            "train loss:0.1271488743644346\n",
            "train loss:0.1792426771755642\n",
            "train loss:0.2742002984754143\n",
            "train loss:0.2969429298483286\n",
            "train loss:0.31770422326718917\n",
            "train loss:0.2773306826529099\n",
            "train loss:0.203678550598693\n",
            "train loss:0.14098269965180682\n",
            "train loss:0.19523333565754702\n",
            "train loss:0.138467826053434\n",
            "train loss:0.265726549848969\n",
            "train loss:0.37605411051951276\n",
            "train loss:0.2942284289697728\n",
            "train loss:0.18303286731183036\n",
            "train loss:0.28074148375906194\n",
            "train loss:0.3145397972153363\n",
            "train loss:0.295280818868886\n",
            "train loss:0.16791332891061528\n",
            "train loss:0.07637134552644533\n",
            "train loss:0.2420650680483023\n",
            "train loss:0.13240000183723857\n",
            "train loss:0.27965364002477305\n",
            "train loss:0.12822830438889954\n",
            "train loss:0.3404994131503877\n",
            "train loss:0.1670567301845604\n",
            "train loss:0.2941616286002192\n",
            "train loss:0.20343577537980828\n",
            "train loss:0.2008689822280868\n",
            "train loss:0.13850091934723036\n",
            "train loss:0.20077706533285236\n",
            "train loss:0.35264135337555214\n",
            "train loss:0.11859700015918032\n",
            "train loss:0.08741946809846543\n",
            "train loss:0.210281753910487\n",
            "train loss:0.12204723994742667\n",
            "train loss:0.1752936290614681\n",
            "train loss:0.16997468498337054\n",
            "train loss:0.1357118830630053\n",
            "train loss:0.12826450993487432\n",
            "train loss:0.2777120939101784\n",
            "train loss:0.09451949812817047\n",
            "train loss:0.17505840325783006\n",
            "train loss:0.10883041578694928\n",
            "train loss:0.19304459803758034\n",
            "train loss:0.15223977313787473\n",
            "train loss:0.13152866162132687\n",
            "train loss:0.09052211966896342\n",
            "train loss:0.09253499263216039\n",
            "train loss:0.11611726821906732\n",
            "train loss:0.10492421518775683\n",
            "train loss:0.06287182011082909\n",
            "train loss:0.05500506258961918\n",
            "train loss:0.39099254330027416\n",
            "train loss:0.15829106720873068\n",
            "train loss:0.2279983699194683\n",
            "train loss:0.1468434644282568\n",
            "train loss:0.08084371451014258\n",
            "train loss:0.11778412764815292\n",
            "train loss:0.06616591742318703\n",
            "train loss:0.06476508202362809\n",
            "train loss:0.13473077311272422\n",
            "train loss:0.09264637435922786\n",
            "train loss:0.10556571805430076\n",
            "train loss:0.07988798908258812\n",
            "train loss:0.05480533689927798\n",
            "train loss:0.0455164369997573\n",
            "train loss:0.07093216616249441\n",
            "train loss:0.06505602472195643\n",
            "train loss:0.0760565018529605\n",
            "train loss:0.06829057605441248\n",
            "train loss:0.11430987427435932\n",
            "train loss:0.16077226314280454\n",
            "train loss:0.08302981624929667\n",
            "train loss:0.10701730908466137\n",
            "train loss:0.19217338102069964\n",
            "train loss:0.2159128041535252\n",
            "train loss:0.06911521050288125\n",
            "train loss:0.15887225376023156\n",
            "train loss:0.05624231901278764\n",
            "train loss:0.056472637764530195\n",
            "train loss:0.062002755698071066\n",
            "train loss:0.048332789164505945\n",
            "train loss:0.07097684745012446\n",
            "train loss:0.1745595669053822\n",
            "train loss:0.06759913698447431\n",
            "train loss:0.0653059653321859\n",
            "train loss:0.033363098178416616\n",
            "train loss:0.1863721199996899\n",
            "train loss:0.05846112621510104\n",
            "train loss:0.039192937231324346\n",
            "train loss:0.05287054623327221\n",
            "train loss:0.20964503100588788\n",
            "train loss:0.04315113525417162\n",
            "train loss:0.08219249789613808\n",
            "train loss:0.058024994255788304\n",
            "train loss:0.0768597819679901\n",
            "train loss:0.23947429182911123\n",
            "train loss:0.1053647629882774\n",
            "train loss:0.09999423720151931\n",
            "train loss:0.028422570990895478\n",
            "train loss:0.10357212687423964\n",
            "train loss:0.10006273410975874\n",
            "train loss:0.05567891774428974\n",
            "train loss:0.052402093940457375\n",
            "train loss:0.06591713953474083\n",
            "train loss:0.043044811659205\n",
            "train loss:0.08622102310540536\n",
            "train loss:0.034861672984037244\n",
            "train loss:0.07532307778951669\n",
            "train loss:0.040513478101233666\n",
            "train loss:0.07254570527986555\n",
            "train loss:0.041329904954308334\n",
            "train loss:0.06317459677124723\n",
            "train loss:0.043549413964697484\n",
            "train loss:0.03424784712221302\n",
            "train loss:0.02593915746502392\n",
            "train loss:0.05552832962653749\n",
            "train loss:0.044754370072409164\n",
            "train loss:0.13505332719404053\n",
            "train loss:0.060814422510149097\n",
            "train loss:0.0558379439354767\n",
            "train loss:0.11880911074727442\n",
            "train loss:0.031931668352789155\n",
            "train loss:0.04501688959595101\n",
            "train loss:0.0285205984114267\n",
            "train loss:0.1820012415572188\n",
            "train loss:0.08880124679382465\n",
            "train loss:0.06970430768395136\n",
            "train loss:0.05112821064368475\n",
            "train loss:0.1312307741534575\n",
            "train loss:0.0705169730259252\n",
            "train loss:0.02663137174968774\n",
            "train loss:0.11012133423761229\n",
            "train loss:0.1512589261529127\n",
            "train loss:0.0638940406933028\n",
            "train loss:0.0778346464291236\n",
            "train loss:0.03406194618199611\n",
            "train loss:0.01728007537976128\n",
            "train loss:0.08700027333615123\n",
            "train loss:0.47705730714361994\n",
            "train loss:0.07356985046342748\n",
            "train loss:0.03521305771511166\n",
            "train loss:0.03414380458307253\n",
            "train loss:0.042690290723771264\n",
            "train loss:0.03851982933636747\n",
            "train loss:0.037013832047376995\n",
            "train loss:0.0206319877979968\n",
            "train loss:0.060281607600662604\n",
            "train loss:0.06060974051827615\n",
            "train loss:0.13620463297233243\n",
            "train loss:0.06380274635947085\n",
            "train loss:0.06030996613873389\n",
            "train loss:0.02137798248604855\n",
            "train loss:0.026959054305754706\n",
            "train loss:0.0739878975608769\n",
            "train loss:0.03762236792555914\n",
            "train loss:0.09570232946362074\n",
            "train loss:0.0251510528103129\n",
            "train loss:0.0806177279170365\n",
            "train loss:0.06444314937311615\n",
            "train loss:0.030274487723342833\n",
            "train loss:0.17911277844152954\n",
            "train loss:0.03969371151882825\n",
            "train loss:0.03231336879527198\n",
            "train loss:0.39626955703160305\n",
            "train loss:0.016431173723505128\n",
            "train loss:0.06126543500967179\n",
            "train loss:0.03228683266408284\n",
            "train loss:0.030436105790349825\n",
            "train loss:0.027709655091576534\n",
            "train loss:0.056994609380581554\n",
            "train loss:0.03810689267304493\n",
            "train loss:0.3055509290508111\n",
            "train loss:0.3043417459625437\n",
            "train loss:0.042193976414050996\n",
            "train loss:0.044077564443815165\n",
            "train loss:0.033092449926202264\n",
            "train loss:0.04014548914717826\n",
            "train loss:0.0674244981436066\n",
            "train loss:0.036404946790878585\n",
            "train loss:0.06751491405368704\n",
            "train loss:0.021731452564475638\n",
            "train loss:0.04997697435351209\n",
            "train loss:0.0668665527876035\n",
            "train loss:0.020691974857728234\n",
            "train loss:0.04878986263678183\n",
            "train loss:0.020833771741636014\n",
            "train loss:0.08793710044137736\n",
            "train loss:0.014285094448647784\n",
            "train loss:0.02419704337676046\n",
            "train loss:0.03241028063070513\n",
            "train loss:0.07499954314014229\n",
            "train loss:0.032804017929043\n",
            "train loss:0.02251490800833738\n",
            "train loss:0.023717657462886628\n",
            "train loss:0.040171920696396086\n",
            "train loss:0.023786154379344206\n",
            "train loss:0.023723449461679556\n",
            "train loss:0.014408318864380364\n",
            "train loss:0.020814895586319923\n",
            "train loss:0.024427311464906974\n",
            "train loss:0.04413047816335158\n",
            "train loss:0.03893619768144729\n",
            "train loss:0.28283631884802884\n",
            "train loss:0.02149738468206936\n",
            "train loss:0.31516898395331405\n",
            "train loss:0.015000550488532728\n",
            "train loss:0.024564495087319872\n",
            "train loss:0.028303334565746865\n",
            "train loss:0.03788923896412362\n",
            "train loss:0.037805378561655285\n",
            "train loss:0.03713929045436293\n",
            "train loss:0.01738957268713151\n",
            "train loss:0.0058339306943682445\n",
            "train loss:0.022656037356736214\n",
            "train loss:0.02194963889526524\n",
            "train loss:0.024503197740310546\n",
            "train loss:0.027369671671724837\n",
            "train loss:0.01594358601179172\n",
            "train loss:0.010932866136955464\n",
            "train loss:0.026093799251042597\n",
            "train loss:0.07231892886354795\n",
            "train loss:0.013764297358527001\n",
            "train loss:0.11232149604588272\n",
            "train loss:0.11431054683461958\n",
            "train loss:0.013797850726489022\n",
            "train loss:0.021481753262661663\n",
            "train loss:0.06100375875741336\n",
            "train loss:0.020429800424266623\n",
            "train loss:0.2726048124313944\n",
            "train loss:0.04111882683849491\n",
            "train loss:0.016937255280611643\n",
            "train loss:0.020271320715902666\n",
            "train loss:0.007279271665267269\n",
            "train loss:0.02771271448531063\n",
            "train loss:0.009209184521319823\n",
            "train loss:0.019074967782631416\n",
            "train loss:0.013742324702542563\n",
            "train loss:0.012218596040580388\n",
            "train loss:0.007881768088375082\n",
            "train loss:0.03086962950873498\n",
            "train loss:0.01323721294986413\n",
            "train loss:0.01951442247493175\n",
            "train loss:0.09512834329881484\n",
            "train loss:0.009250023816826195\n",
            "train loss:0.010521046830565836\n",
            "train loss:0.0338364013323581\n",
            "train loss:0.023740561966142184\n",
            "train loss:0.012022699340315047\n",
            "train loss:0.020413275704194305\n",
            "train loss:0.008409811355107346\n",
            "train loss:0.13680810924195555\n",
            "train loss:0.011853106650943478\n",
            "train loss:0.0631260622836313\n",
            "train loss:0.009526676611998212\n",
            "train loss:0.02204420533367859\n",
            "train loss:0.012673305922850615\n",
            "train loss:0.077419136331052\n",
            "train loss:0.01596580411998714\n",
            "train loss:0.015380691163437401\n",
            "train loss:0.068103806306748\n",
            "train loss:0.013333957464862704\n",
            "train loss:0.01586308699012733\n",
            "train loss:0.011535638273456552\n",
            "train loss:0.008034653275328287\n",
            "train loss:0.018796231660838533\n",
            "train loss:0.012235543737161311\n",
            "train loss:0.016263517333715092\n",
            "train loss:0.021420365702463424\n",
            "train loss:0.01566106748802856\n",
            "train loss:0.01166576243143177\n",
            "train loss:0.011962119048597512\n",
            "train loss:0.010180717132555358\n",
            "train loss:0.009670991886052062\n",
            "train loss:0.010933055244769703\n",
            "train loss:0.00711168260096805\n",
            "train loss:0.03476401140055517\n",
            "train loss:0.009004289632532879\n",
            "train loss:0.007264789861502206\n",
            "train loss:0.008063145042418238\n",
            "train loss:0.012871711706301027\n",
            "train loss:0.008935987681555157\n",
            "train loss:0.013890975333930272\n",
            "train loss:0.008655097907583915\n",
            "train loss:0.04256156958229035\n",
            "train loss:0.009434559231569637\n",
            "train loss:0.04492733333715107\n",
            "train loss:0.009916012221301479\n",
            "train loss:0.011630045746144216\n",
            "train loss:0.1238378575764341\n",
            "train loss:0.06709595426585141\n",
            "train loss:0.009394937682443026\n",
            "train loss:0.013465659828038505\n",
            "train loss:0.024472975702084936\n",
            "train loss:0.016203366191936245\n",
            "train loss:0.01563538922258121\n",
            "train loss:0.0219960569032587\n",
            "train loss:0.010223343638697216\n",
            "train loss:0.010555641780417206\n",
            "train loss:0.036781551038457413\n",
            "train loss:0.008268957897658868\n",
            "train loss:0.028948914579295826\n",
            "train loss:0.014099675496419242\n",
            "train loss:0.011583525368527681\n",
            "train loss:0.3339215004919982\n",
            "train loss:0.008148025450638332\n",
            "train loss:0.03575989810742626\n",
            "train loss:0.013458431275526813\n",
            "train loss:0.01847049586412329\n",
            "train loss:0.018173487574662\n",
            "train loss:0.010489188112315393\n",
            "train loss:0.012971488095120203\n",
            "train loss:0.2203565487408959\n",
            "train loss:0.018703614521623467\n",
            "train loss:0.008126188639196875\n",
            "train loss:0.016094549576201794\n",
            "train loss:0.08420272116224696\n",
            "train loss:0.027432254734365044\n",
            "train loss:0.01401205266545827\n",
            "train loss:0.007591237396448272\n",
            "train loss:0.010413897571856238\n",
            "train loss:0.007528985025787881\n",
            "train loss:0.22820472195846553\n",
            "train loss:0.023425817239774792\n",
            "train loss:0.014036543716523808\n",
            "train loss:0.01767623283602723\n",
            "train loss:0.004045352996637265\n",
            "train loss:0.028031767016386606\n",
            "train loss:0.00601889270419977\n",
            "train loss:0.008895804725865215\n",
            "train loss:0.00701292650150202\n",
            "train loss:0.008802136306909517\n",
            "train loss:0.005965554055673925\n",
            "train loss:0.017067420685675655\n",
            "train loss:0.022799239943706605\n",
            "train loss:0.004446474416440934\n",
            "train loss:0.007778701448953785\n",
            "train loss:0.01573736026246083\n",
            "train loss:0.015002238583944805\n",
            "train loss:0.019634751928179067\n",
            "train loss:0.009922681434166554\n",
            "train loss:0.007564338537684224\n",
            "train loss:0.05923945408884489\n",
            "train loss:0.02046481866233916\n",
            "train loss:0.011578302467828705\n",
            "train loss:0.0130729169902889\n",
            "train loss:0.012151002803608354\n",
            "train loss:0.015283655105182132\n",
            "train loss:0.00922541859760223\n",
            "train loss:0.04876782762855206\n",
            "train loss:0.1345176057387021\n",
            "train loss:0.01942454553352977\n",
            "train loss:0.012970062180628754\n",
            "train loss:0.01676628475675898\n",
            "train loss:0.006359679292127738\n",
            "train loss:0.006884973238408664\n",
            "train loss:0.007778206384606019\n",
            "train loss:0.01720767469677498\n",
            "train loss:0.01621613522407628\n",
            "train loss:0.024697286379633962\n",
            "train loss:0.16963316998353456\n",
            "train loss:0.008355041723875347\n",
            "train loss:0.006920617301011072\n",
            "train loss:0.005088096157697213\n",
            "train loss:0.006677294146990572\n",
            "train loss:0.012076023723817577\n",
            "train loss:0.005182734165194443\n",
            "train loss:0.012663648710697606\n",
            "train loss:0.01045810174021256\n",
            "train loss:0.02376651026636542\n",
            "train loss:0.01506365437265257\n",
            "train loss:0.010817219967551334\n",
            "train loss:0.2698322325294398\n",
            "train loss:0.009604928941777377\n",
            "train loss:0.0037666036972310545\n",
            "train loss:0.006715669760271242\n",
            "train loss:0.007372627442222274\n",
            "train loss:0.006265616794172115\n",
            "train loss:0.009787014572102772\n",
            "train loss:0.007003108047699664\n",
            "train loss:0.007755121989573547\n",
            "train loss:0.01790551506507996\n",
            "train loss:0.013542242077831671\n",
            "train loss:0.05086353486306594\n",
            "train loss:0.018166259998937256\n",
            "train loss:0.017990598393714002\n",
            "train loss:0.0056567017078047515\n",
            "train loss:0.0044912823501023075\n",
            "train loss:0.009829043369968106\n",
            "train loss:0.07061368686866447\n",
            "train loss:0.0028018748512120503\n",
            "train loss:0.026928205498240692\n",
            "train loss:0.00722876177337013\n",
            "train loss:0.01542016903974772\n",
            "train loss:0.007746373888322864\n",
            "train loss:0.01556260895889675\n",
            "train loss:0.013342623842156443\n",
            "train loss:0.005105066339427524\n",
            "train loss:0.03149116962658018\n",
            "train loss:0.007858059484569784\n",
            "train loss:0.005508902883555511\n",
            "train loss:0.006762310008923168\n",
            "train loss:0.00977151493526715\n",
            "train loss:0.013476344757919586\n",
            "train loss:0.002890682806684805\n",
            "train loss:0.006921745472737358\n",
            "train loss:0.0034868430322979567\n",
            "train loss:0.005576719060327203\n",
            "train loss:0.005169649189248146\n",
            "train loss:0.005147767341330718\n",
            "train loss:0.005643720474174933\n",
            "train loss:0.0037963074059228774\n",
            "train loss:0.006835214179654448\n",
            "train loss:0.003535197926210879\n",
            "train loss:0.005592690083084818\n",
            "train loss:0.009020953514232728\n",
            "train loss:0.030422426629345335\n",
            "train loss:0.005912096426369123\n",
            "train loss:0.006245836289098228\n",
            "train loss:0.005317547110182575\n",
            "train loss:0.4674089936413311\n",
            "train loss:0.004203231613534947\n",
            "train loss:0.007586126218437572\n",
            "train loss:0.01766905812447455\n",
            "train loss:0.007969250112235704\n",
            "train loss:0.0036467546564525125\n",
            "train loss:0.007730028346434226\n",
            "train loss:0.014116952629754868\n",
            "train loss:0.00465753057132167\n",
            "train loss:0.00625978767899949\n",
            "train loss:0.013503636443252998\n",
            "train loss:0.005636859085602526\n",
            "train loss:0.010243632579606523\n",
            "train loss:0.06629757918427794\n",
            "train loss:0.026422747594244077\n",
            "train loss:0.004212433722540259\n",
            "train loss:0.00795938083462667\n",
            "train loss:0.007583635619883232\n",
            "train loss:0.006403260771723738\n",
            "train loss:0.005784953221756931\n",
            "train loss:0.06593073761829249\n",
            "train loss:0.006708431215269437\n",
            "train loss:0.012400600229025848\n",
            "train loss:0.009151366001678055\n",
            "train loss:0.009940431833758312\n",
            "train loss:0.0644117570701827\n",
            "train loss:0.005314435102163216\n",
            "train loss:0.004843172212248911\n",
            "train loss:0.004313697791314795\n",
            "train loss:0.0030939628012981633\n",
            "train loss:0.16791378611140056\n",
            "train loss:0.004809979730654874\n",
            "train loss:0.0037983632473504204\n",
            "train loss:0.004429668860022173\n",
            "train loss:0.00495668223411737\n",
            "train loss:0.01275951905805284\n",
            "train loss:0.005466790976976256\n",
            "train loss:0.0045564547394965\n",
            "train loss:0.04319982340093545\n",
            "train loss:0.005513649822835794\n",
            "train loss:0.040519943299563765\n",
            "train loss:0.008347641311559859\n",
            "train loss:0.003049618572589341\n",
            "train loss:0.003911624663219219\n",
            "train loss:0.00473016121390649\n",
            "train loss:0.0037700478137327183\n",
            "train loss:0.0039315897395812505\n",
            "train loss:0.007574650211536284\n",
            "train loss:0.0028075958035982972\n",
            "train loss:0.0042293913177678406\n",
            "train loss:0.005909011629111598\n",
            "train loss:0.0037162412884135573\n",
            "train loss:0.0029797559527931896\n",
            "train loss:0.005040650697845224\n",
            "train loss:0.004710022558274883\n",
            "train loss:0.003086104450322948\n",
            "train loss:0.0030693212203309556\n",
            "train loss:0.006655667707248836\n",
            "train loss:0.003789952992581433\n",
            "train loss:0.003372703704767198\n",
            "train loss:0.008670271470215079\n",
            "train loss:0.005175629487436388\n",
            "train loss:0.003205838522283113\n",
            "train loss:0.005747526320110789\n",
            "train loss:0.0037995782396356773\n",
            "train loss:0.002279723514373897\n",
            "train loss:0.0033035430169652954\n",
            "train loss:0.0024853333652000375\n",
            "train loss:0.00416856413359477\n",
            "train loss:0.09684671705016118\n",
            "train loss:0.09933688669824237\n",
            "train loss:0.008065338458299973\n",
            "train loss:0.01562920662902051\n",
            "train loss:0.0118071564926809\n",
            "train loss:0.002312602104444469\n",
            "train loss:0.0035858845346158803\n",
            "train loss:0.004904689872315536\n",
            "train loss:0.004999903593330641\n",
            "train loss:0.004408368542879251\n",
            "train loss:0.004170355753246384\n",
            "train loss:0.029129346592312148\n",
            "train loss:0.0058012329141428395\n",
            "train loss:0.009859385537075573\n",
            "train loss:0.00854643081429178\n",
            "train loss:0.006382175240927147\n",
            "train loss:0.005048547748067275\n",
            "train loss:0.005281665886885188\n",
            "train loss:0.0055528476634319765\n",
            "train loss:0.004065732141571112\n",
            "=============== Final Train Accuracy ===============\n",
            "train acc:\n",
            "0.9866666666666667\n",
            "검증 데이터로 성능 평가 중...\n",
            "검증 정확도: 0.0000\n",
            "최종 훈련 정확도: 0.9960\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import urllib.request\n",
        "import os\n",
        "import os.path\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x.T\n",
        "        x = x - np.max(x, axis=0)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "        return y.T\n",
        "\n",
        "    x = x - np.max(x) # 오버플로 대책\n",
        "    return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "\n",
        "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "\n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
        "\n",
        "\n",
        "def smooth_curve(x):\n",
        "    \"\"\"손실 함수의 그래프를 매끄럽게 하기 위해 사용\n",
        "\n",
        "    참고：http://glowingpython.blogspot.jp/2012/02/convolution-with-numpy.html\n",
        "    \"\"\"\n",
        "    window_len = 11\n",
        "    s = np.r_[x[window_len-1:0:-1], x, x[-1:-window_len:-1]]\n",
        "    w = np.kaiser(window_len, 2)\n",
        "    y = np.convolve(w/w.sum(), s, mode='valid')\n",
        "    return y[5:len(y)-5]\n",
        "\n",
        "\n",
        "def shuffle_dataset(x, t):\n",
        "    \"\"\"데이터셋을 뒤섞는다.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : 훈련 데이터\n",
        "    t : 정답 레이블\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    x, t : 뒤섞은 훈련 데이터와 정답 레이블\n",
        "    \"\"\"\n",
        "    permutation = np.random.permutation(x.shape[0])\n",
        "    x = x[permutation,:] if x.ndim == 2 else x[permutation,:,:,:]\n",
        "    t = t[permutation]\n",
        "\n",
        "    return x, t\n",
        "\n",
        "def conv_output_size(input_size, filter_size, stride=1, pad=0):\n",
        "    return (input_size + 2*pad - filter_size) / stride + 1\n",
        "\n",
        "\n",
        "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n",
        "    filter_h : 필터의 높이\n",
        "    filter_w : 필터의 너비\n",
        "    stride : 스트라이드\n",
        "    pad : 패딩\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    col : 2차원 배열\n",
        "    \"\"\"\n",
        "    N, C, H, W = input_data.shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "\n",
        "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
        "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
        "\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
        "\n",
        "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
        "    return col\n",
        "\n",
        "\n",
        "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"(im2col과 반대) 2차원 배열을 입력받아 다수의 이미지 묶음으로 변환한다.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    col : 2차원 배열(입력 데이터)\n",
        "    input_shape : 원래 이미지 데이터의 형상（예：(10, 1, 28, 28)）\n",
        "    filter_h : 필터의 높이\n",
        "    filter_w : 필터의 너비\n",
        "    stride : 스트라이드\n",
        "    pad : 패딩\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    img : 변환된 이미지들\n",
        "    \"\"\"\n",
        "    N, C, H, W = input_shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
        "\n",
        "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
        "\n",
        "    return img[:, :, pad:H + pad, pad:W + pad]\n",
        "\n",
        "import numpy as np\n",
        "#from common.functions import *\n",
        "#from common.util import im2col, col2im\n",
        "\n",
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = sigmoid(x)\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "\n",
        "        self.x = None\n",
        "        self.original_x_shape = None\n",
        "        # 가중치와 편향 매개변수의 미분\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 텐서 대응\n",
        "        self.original_x_shape = x.shape\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        self.x = x\n",
        "\n",
        "        out = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "\n",
        "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
        "        return dx\n",
        "\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None # 손실함수\n",
        "        self.y = None    # softmax의 출력\n",
        "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
        "            dx = (self.y - self.t) / batch_size\n",
        "        else:\n",
        "            dx = self.y.copy()\n",
        "            dx[np.arange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Dropout:\n",
        "    \"\"\"\n",
        "    http://arxiv.org/abs/1207.0580\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout_ratio=0.5):\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        if train_flg:\n",
        "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
        "            return x * self.mask\n",
        "        else:\n",
        "            return x * (1.0 - self.dropout_ratio)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask\n",
        "\n",
        "\n",
        "class BatchNormalization:\n",
        "    \"\"\"\n",
        "    http://arxiv.org/abs/1502.03167\n",
        "    \"\"\"\n",
        "    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n",
        "        self.gamma = gamma\n",
        "        self.beta = beta\n",
        "        self.momentum = momentum\n",
        "        self.input_shape = None # 합성곱 계층은 4차원, 완전연결 계층은 2차원\n",
        "\n",
        "        # 시험할 때 사용할 평균과 분산\n",
        "        self.running_mean = running_mean\n",
        "        self.running_var = running_var\n",
        "\n",
        "        # backward 시에 사용할 중간 데이터\n",
        "        self.batch_size = None\n",
        "        self.xc = None\n",
        "        self.std = None\n",
        "        self.dgamma = None\n",
        "        self.dbeta = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        self.input_shape = x.shape\n",
        "        if x.ndim != 2:\n",
        "            N, C, H, W = x.shape\n",
        "            x = x.reshape(N, -1)\n",
        "\n",
        "        out = self.__forward(x, train_flg)\n",
        "\n",
        "        return out.reshape(*self.input_shape)\n",
        "\n",
        "    def __forward(self, x, train_flg):\n",
        "        if self.running_mean is None:\n",
        "            N, D = x.shape\n",
        "            self.running_mean = np.zeros(D)\n",
        "            self.running_var = np.zeros(D)\n",
        "\n",
        "        if train_flg:\n",
        "            mu = x.mean(axis=0)\n",
        "            xc = x - mu\n",
        "            var = np.mean(xc**2, axis=0)\n",
        "            std = np.sqrt(var + 10e-7)\n",
        "            xn = xc / std\n",
        "\n",
        "            self.batch_size = x.shape[0]\n",
        "            self.xc = xc\n",
        "            self.xn = xn\n",
        "            self.std = std\n",
        "            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
        "            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var\n",
        "        else:\n",
        "            xc = x - self.running_mean\n",
        "            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
        "\n",
        "        out = self.gamma * xn + self.beta\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        if dout.ndim != 2:\n",
        "            N, C, H, W = dout.shape\n",
        "            dout = dout.reshape(N, -1)\n",
        "\n",
        "        dx = self.__backward(dout)\n",
        "\n",
        "        dx = dx.reshape(*self.input_shape)\n",
        "        return dx\n",
        "\n",
        "    def __backward(self, dout):\n",
        "        dbeta = dout.sum(axis=0)\n",
        "        dgamma = np.sum(self.xn * dout, axis=0)\n",
        "        dxn = self.gamma * dout\n",
        "        dxc = dxn / self.std\n",
        "        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
        "        dvar = 0.5 * dstd / self.std\n",
        "        dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
        "        dmu = np.sum(dxc, axis=0)\n",
        "        dx = dxc - dmu / self.batch_size\n",
        "\n",
        "        self.dgamma = dgamma\n",
        "        self.dbeta = dbeta\n",
        "\n",
        "        return dx\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def _numerical_gradient_1d(f, x):\n",
        "    h = 1e-4 # 0.0001\n",
        "    grad = np.zeros_like(x)\n",
        "\n",
        "    for idx in range(x.size):\n",
        "        tmp_val = x[idx]\n",
        "        x[idx] = float(tmp_val) + h\n",
        "        fxh1 = f(x) # f(x+h)\n",
        "\n",
        "        x[idx] = tmp_val - h\n",
        "        fxh2 = f(x) # f(x-h)\n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "\n",
        "        x[idx] = tmp_val # 값 복원\n",
        "\n",
        "    return grad\n",
        "\n",
        "def numerical_gradient_2d(f, X):\n",
        "    if X.ndim == 1:\n",
        "        return _numerical_gradient_1d(f, X)\n",
        "    else:\n",
        "        grad = np.zeros_like(X)\n",
        "\n",
        "        for idx, x in enumerate(X):\n",
        "            grad[idx] = _numerical_gradient_1d(f, x)\n",
        "\n",
        "        return grad\n",
        "\n",
        "def numerical_gradient(f, x):\n",
        "    h = 1e-4 # 0.0001\n",
        "    grad = np.zeros_like(x)\n",
        "\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        idx = it.multi_index\n",
        "        tmp_val = x[idx]\n",
        "        x[idx] = float(tmp_val) + h\n",
        "        fxh1 = f(x) # f(x+h)\n",
        "\n",
        "        x[idx] = tmp_val - h\n",
        "        fxh2 = f(x) # f(x-h)\n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "\n",
        "        x[idx] = tmp_val # 값 복원\n",
        "        it.iternext()\n",
        "\n",
        "    return grad\n",
        "\n",
        "class SGD:\n",
        "    \"\"\"확률적 경사 하강법（Stochastic Gradient Descent）\"\"\"\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        for key in params.keys():\n",
        "            params[key] -= self.lr * grads[key]\n",
        "\n",
        "class Momentum:\n",
        "    \"\"\"모멘텀 SGD\"\"\"\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = {}\n",
        "            for key, val in params.items():\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]\n",
        "            params[key] += self.v[key]\n",
        "\n",
        "class Nesterov:\n",
        "    \"\"\"Nesterov's Accelerated Gradient (http://arxiv.org/abs/1212.0901)\"\"\"\n",
        "    # NAG는 모멘텀에서 한 단계 발전한 방법이다. (http://newsight.tistory.com/224)\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = {}\n",
        "            for key, val in params.items():\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.v[key] *= self.momentum\n",
        "            self.v[key] -= self.lr * grads[key]\n",
        "            params[key] += self.momentum * self.momentum * self.v[key]\n",
        "            params[key] -= (1 + self.momentum) * self.lr * grads[key]\n",
        "\n",
        "class AdaGrad:\n",
        "    \"\"\"AdaGrad\"\"\"\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        self.h = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = {}\n",
        "            for key, val in params.items():\n",
        "                self.h[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.h[key] += grads[key] * grads[key]\n",
        "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
        "\n",
        "class RMSprop:\n",
        "    \"\"\"RMSprop\"\"\"\n",
        "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
        "        self.lr = lr\n",
        "        self.decay_rate = decay_rate\n",
        "        self.h = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = {}\n",
        "            for key, val in params.items():\n",
        "                self.h[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.h[key] *= self.decay_rate\n",
        "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
        "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
        "\n",
        "class Adam:\n",
        "    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.iter = 0\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m, self.v = {}, {}\n",
        "            for key, val in params.items():\n",
        "                self.m[key] = np.zeros_like(val)\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "\n",
        "        self.iter += 1\n",
        "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
        "\n",
        "        for key in params.keys():\n",
        "            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
        "            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
        "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
        "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
        "\n",
        "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
        "\n",
        "            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n",
        "            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n",
        "            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)\n",
        "\n",
        "import numpy as np\n",
        "#from common.optimizer import *\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\" 신경망 훈련을 대신 해주는 클래스\n",
        "    \"\"\"\n",
        "    def __init__(self, network, x_train, t_train,\n",
        "                 epochs=20, mini_batch_size=100,\n",
        "                 optimizer='SGD', optimizer_param={'lr':0.01},\n",
        "                 evaluate_sample_num_per_epoch=None, verbose=True):\n",
        "        self.network = network\n",
        "        self.verbose = verbose\n",
        "        self.x_train = x_train\n",
        "        self.t_train = t_train\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = mini_batch_size\n",
        "        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n",
        "\n",
        "        # optimzer\n",
        "        optimizer_class_dict = {'sgd':SGD, 'momentum':Momentum, 'nesterov':Nesterov,\n",
        "                                'adagrad':AdaGrad, 'rmsprpo':RMSprop, 'adam':Adam}\n",
        "        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n",
        "\n",
        "        self.train_size = x_train.shape[0]\n",
        "        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\n",
        "        self.max_iter = int(epochs * self.iter_per_epoch)\n",
        "        self.current_iter = 0\n",
        "        self.current_epoch = 0\n",
        "\n",
        "        self.train_loss_list = []\n",
        "        self.train_acc_list = []\n",
        "        self.test_acc_list = []\n",
        "\n",
        "    def train_step(self):\n",
        "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
        "        x_batch = self.x_train[batch_mask]\n",
        "        t_batch = self.t_train[batch_mask]\n",
        "\n",
        "        grads = self.network.gradient(x_batch, t_batch)\n",
        "        self.optimizer.update(self.network.params, grads)\n",
        "\n",
        "        loss = self.network.loss(x_batch, t_batch)\n",
        "        self.train_loss_list.append(loss)\n",
        "        if self.verbose: print(\"train loss:\" + str(loss))\n",
        "\n",
        "        if self.current_iter % self.iter_per_epoch == 0:\n",
        "            self.current_epoch += 1\n",
        "\n",
        "            x_train_sample, t_train_sample = self.x_train, self.t_train\n",
        "            if not self.evaluate_sample_num_per_epoch is None:\n",
        "                t = self.evaluate_sample_num_per_epoch\n",
        "                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n",
        "\n",
        "            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n",
        "            self.train_acc_list.append(train_acc)\n",
        "\n",
        "        self.current_iter += 1\n",
        "\n",
        "    def train(self):\n",
        "        for i in range(self.max_iter):\n",
        "            self.train_step()\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"=============== Final Train Accuracy ===============\")\n",
        "            print(\"train acc:\" )\n",
        "            print(self.train_acc_list[-1])\n",
        "\n",
        "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n",
        "    filter_h : 필터의 높이\n",
        "    filter_w : 필터의 너비\n",
        "    stride : 스트라이드\n",
        "    pad : 패딩\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    col : 2차원 배열\n",
        "    \"\"\"\n",
        "    N, C, H, W = input_data.shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "\n",
        "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
        "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
        "\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
        "\n",
        "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
        "    return col\n",
        "\n",
        "class Convolution:\n",
        "    def __init__(self, W, b, stride=1, pad=0):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "        # 중간 데이터（backward 시 사용）\n",
        "        self.x = None\n",
        "        self.col = None\n",
        "        self.col_W = None\n",
        "\n",
        "        # 가중치와 편향 매개변수의 기울기\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
        "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
        "\n",
        "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "        col_W = self.W.reshape(FN, -1).T\n",
        "\n",
        "        out = np.dot(col, col_W) + self.b\n",
        "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.col = col\n",
        "        self.col_W = col_W\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
        "\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        self.dW = np.dot(self.col.T, dout)\n",
        "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
        "\n",
        "        dcol = np.dot(dout, self.col_W.T)\n",
        "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
        "\n",
        "        return dx\n",
        "\n",
        "class Pooling:\n",
        "    def __init__(self, pool_h, pool_w, stride=2, pad=0):\n",
        "        self.pool_h = pool_h\n",
        "        self.pool_w = pool_w\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "        self.x = None\n",
        "        self.arg_max = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
        "\n",
        "        arg_max = np.argmax(col, axis=1)\n",
        "        out = np.max(col, axis=1)\n",
        "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.arg_max = arg_max\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout = dout.transpose(0, 2, 3, 1)\n",
        "\n",
        "        pool_size = self.pool_h * self.pool_w\n",
        "        dmax = np.zeros((dout.size, pool_size))\n",
        "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
        "        dmax = dmax.reshape(dout.shape + (pool_size,))\n",
        "\n",
        "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
        "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "\n",
        "        return dx\n",
        "\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class SimpleConvNet:\n",
        "    \"\"\"단순한 합성곱 신경망\n",
        "\n",
        "    conv - relu - pool - affine - relu - affine - softmax\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
        "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
        "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
        "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
        "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
        "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
        "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=(1, 28, 28),\n",
        "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
        "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
        "        filter_num = conv_param['filter_num']\n",
        "        filter_size = conv_param['filter_size']\n",
        "        filter_pad = conv_param['pad']\n",
        "        filter_stride = conv_param['stride']\n",
        "        input_size = input_dim[1]\n",
        "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
        "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
        "\n",
        "        # 가중치 초기화\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * \\\n",
        "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
        "        self.params['b1'] = np.zeros(filter_num)\n",
        "        self.params['W2'] = weight_init_std * \\\n",
        "                            np.random.randn(pool_output_size, hidden_size)\n",
        "        self.params['b2'] = np.zeros(hidden_size)\n",
        "        self.params['W3'] = weight_init_std * \\\n",
        "                            np.random.randn(hidden_size, output_size)\n",
        "        self.params['b3'] = np.zeros(output_size)\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
        "                                           conv_param['stride'], conv_param['pad'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
        "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
        "        self.layers['Relu2'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
        "\n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        \"\"\"손실 함수를 구한다.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        \"\"\"\n",
        "        y = self.predict(x)\n",
        "        return self.last_layer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t, batch_size=100):\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "\n",
        "        acc = 0.0\n",
        "\n",
        "        for i in range(int(x.shape[0] / batch_size)):\n",
        "            tx = x[i*batch_size:(i+1)*batch_size]\n",
        "            tt = t[i*batch_size:(i+1)*batch_size]\n",
        "            y = self.predict(tx)\n",
        "            y = np.argmax(y, axis=1)\n",
        "            acc += np.sum(y == tt)\n",
        "\n",
        "        return acc / x.shape[0]\n",
        "\n",
        "    def numerical_gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다（수치미분）.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        loss_w = lambda w: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        for idx in (1, 2, 3):\n",
        "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
        "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다(오차역전파법).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        # 순전파\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # 역전파\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def save_params(self, file_name=\"params.pkl\"):\n",
        "        params = {}\n",
        "        for key, val in self.params.items():\n",
        "            params[key] = val\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "    def load_params(self, file_name=\"params.pkl\"):\n",
        "        with open(os.path.dirname(__file__) + '/' + file_name, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "        for key, val in params.items():\n",
        "            self.params[key] = val\n",
        "\n",
        "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
        "            self.layers[key].W = self.params['W' + str(i+1)]\n",
        "            self.layers[key].b = self.params['b' + str(i+1)]\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "# 데이터 로드 관련 함수\n",
        "def _change_one_hot_label(X, num_classes=11):\n",
        "    T = np.zeros((len(X), num_classes))\n",
        "    for idx, label in enumerate(X):\n",
        "        T[idx, int(label)] = 1\n",
        "    return T\n",
        "\n",
        "def load_labels(label_file, one_hot_label=False, num_classes=11):\n",
        "    \"\"\"\n",
        "    라벨 파일을 읽어 정수 배열 또는 one-hot 배열로 반환\n",
        "    \"\"\"\n",
        "    with open(label_file, 'r') as f:\n",
        "        content = f.read().strip()\n",
        "        # 콤마로 구분\n",
        "        labels = content.split(',')\n",
        "        labels = [int(l) for l in labels if l != '']\n",
        "        labels = np.array(labels)\n",
        "    if one_hot_label:\n",
        "        labels = _change_one_hot_label(labels, num_classes)\n",
        "    return labels\n",
        "\n",
        "def preprocess_images(images):\n",
        "    \"\"\"\n",
        "    이미지 전처리 - 표준화 적용\n",
        "    \"\"\"\n",
        "    processed = []\n",
        "    for img in images:\n",
        "        # 각 이미지별로 표준화\n",
        "        mean = np.mean(img)\n",
        "        std = np.std(img)\n",
        "        if std > 0:\n",
        "            normalized = (img - mean) / std\n",
        "        else:\n",
        "            normalized = img - mean\n",
        "        processed.append(normalized)\n",
        "    return np.array(processed)\n",
        "\n",
        "def augment_training_data(x_train, t_train):\n",
        "    \"\"\"\n",
        "    간단한 데이터 증강 - 좌우 반전, 회전, 노이즈 추가\n",
        "    \"\"\"\n",
        "    augmented_x = []\n",
        "    augmented_t = []\n",
        "\n",
        "    for i in range(len(x_train)):\n",
        "        # 원본 추가\n",
        "        augmented_x.append(x_train[i])\n",
        "        augmented_t.append(t_train[i])\n",
        "\n",
        "        # 좌우 반전 (확률적으로)\n",
        "        if np.random.random() > 0.4:\n",
        "            flipped = np.fliplr(x_train[i, 0])\n",
        "            augmented_x.append(flipped[np.newaxis, :, :])\n",
        "            augmented_t.append(t_train[i])\n",
        "\n",
        "        # 작은 노이즈 추가 (확률적으로)\n",
        "        if np.random.random() > 0.6:\n",
        "            noise = np.random.normal(0, 0.05, x_train[i].shape)\n",
        "            noisy = x_train[i] + noise\n",
        "            augmented_x.append(noisy)\n",
        "            augmented_t.append(t_train[i])\n",
        "\n",
        "    return np.array(augmented_x), np.array(augmented_t)\n",
        "\n",
        "def create_validation_split(x_train, t_train, val_ratio=0.2):\n",
        "    \"\"\"\n",
        "    검증 데이터 분할\n",
        "    \"\"\"\n",
        "    n_val = int(len(x_train) * val_ratio)\n",
        "    indices = np.random.permutation(len(x_train))\n",
        "\n",
        "    val_indices = indices[:n_val]\n",
        "    train_indices = indices[n_val:]\n",
        "\n",
        "    x_val = x_train[val_indices]\n",
        "    t_val = t_train[val_indices]\n",
        "    x_train_new = x_train[train_indices]\n",
        "    t_train_new = t_train[train_indices]\n",
        "\n",
        "    return x_train_new, t_train_new, x_val, t_val\n",
        "\n",
        "def load_images(image_paths, normalize=True, flatten=False):\n",
        "    \"\"\"\n",
        "    이미지 파일 경로 리스트를 받아 (N, 4096) 또는 (N, 1, 64, 64) 형태로 반환\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    list = os.listdir(image_paths)\n",
        "    list.sort()\n",
        "    for i, filename in enumerate(list):\n",
        "        path = image_paths + filename\n",
        "\n",
        "        img = Image.open(path)\n",
        "        # img = img.resize((64, 64))\n",
        "        arr = np.array(img, dtype=np.float32)\n",
        "        if normalize:\n",
        "            arr /= 255.0\n",
        "        if flatten:\n",
        "            arr = arr.flatten()\n",
        "        else:\n",
        "            arr = arr[np.newaxis, :, :]\n",
        "        images.append(arr)\n",
        "    images = np.stack(images, axis=0)\n",
        "    return images\n",
        "\n",
        "# 데이터 로드\n",
        "print(\"데이터 로딩 중...\")\n",
        "x_train = load_images('tr-re-g/')\n",
        "t_train = load_labels('tr-re-g-label')\n",
        "x_test = load_images('te-re-g/')\n",
        "\n",
        "# 랜덤 시드 고정\n",
        "np.random.seed(42)  # 시드값 변경\n",
        "\n",
        "# 데이터 전처리 적용\n",
        "print(\"데이터 전처리 중...\")\n",
        "x_train = preprocess_images(x_train)\n",
        "x_test = preprocess_images(x_test)\n",
        "\n",
        "# 검증 데이터 분할\n",
        "print(\"검증 데이터 생성 중...\")\n",
        "x_train_split, t_train_split, x_val, t_val = create_validation_split(x_train, t_train, val_ratio=0.1)  # 검증 비율 감소\n",
        "\n",
        "# 데이터 증강 (더 많이)\n",
        "print(\"데이터 증강 중...\")\n",
        "x_train_aug, t_train_aug = augment_training_data(x_train_split, t_train_split)\n",
        "\n",
        "# 데이터 셞플\n",
        "x_train_aug, t_train_aug = shuffle_dataset(x_train_aug, t_train_aug)\n",
        "\n",
        "print(f\"원본 훈련 데이터: {x_train_split.shape}\")\n",
        "print(f\"증강된 훈련 데이터: {x_train_aug.shape}\")\n",
        "print(f\"검증 데이터: {x_val.shape}\")\n",
        "\n",
        "# CNN - 파라미터만 수정해서 성능 개선\n",
        "max_epochs = 15  # 에포크 수 더 증가\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1,64,64),\n",
        "                        conv_param = {'filter_num': 32, 'filter_size': 3, 'pad': 1, 'stride': 1},  # 필터 수 더 증가\n",
        "                        hidden_size=512, output_size=11, weight_init_std=0.05)  # 히든 크기 더 증가, 가중치 조정\n",
        "\n",
        "#학습 설정 - 증강된 데이터 사용\n",
        "print(\"모델 훈련 시작...\")\n",
        "trainer = Trainer(network, x_train_aug, t_train_aug,\n",
        "                  epochs=max_epochs, mini_batch_size=32,  # 배치 크기 더 감소\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.0005},  # 학습률 더 감소\n",
        "                  evaluate_sample_num_per_epoch=300)\n",
        "trainer.train() # 학습\n",
        "\n",
        "# 검증 정확도 확인\n",
        "print(\"검증 데이터로 성능 평가 중...\")\n",
        "val_acc = network.accuracy(x_val, t_val)\n",
        "print(f\"검증 정확도: {val_acc:.4f}\")\n",
        "\n",
        "# 최종 훈련 정확도\n",
        "final_train_acc = network.accuracy(x_train_split[:500], t_train_split[:500])  # 일부만 계산 (속도 향상)\n",
        "print(f\"최종 훈련 정확도: {final_train_acc:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "######### 아래는 수정하지 않으셔도 됩니다.  #########\n",
        "y = trainer.network.predict(x_test)\n",
        "y = np.argmax(y, axis=1)\n",
        "\n",
        "submission = pd.read_csv('sample_submission.csv')\n",
        "submission['label'] = y\n",
        "submission['label'] = submission['label'].apply(lambda x : str(x))\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)"
      ]
    }
  ]
}